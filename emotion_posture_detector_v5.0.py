import cv2
import numpy as np
import os
import sys
import time
import socket
from collections import deque, Counter
from threading import Thread, Lock
import tkinter as tk
from tkinter import messagebox, ttk, filedialog
from PIL import Image, ImageDraw, ImageFont, ImageTk
import qrcode
import io
import pyautogui
from pygrabber.dshow_graph import FilterGraph 
import win32gui
import win32clipboard
import win32con
import win32api
from flask import Flask, Response, render_template_string, request
import csv
import datetime
from docx import Document
import requests
import subprocess

SERVER_URL = "https://epd-test.onrender.com/log_incident/"

class_name = False# l·ªõp
SCAN_MODE = "epd_full"# mode g·ª≠i server
ZONE_ID = None# ƒë·∫°i di·ªán ROI

ROI_STATE_TRACKER = {
    "state": None,
    "start_time": None
}

# Setup ƒë∆∞·ªùng d·∫´n
if getattr(sys, 'frozen', False):
    BASE_DIR = sys._MEIPASS
else:
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))

icon_path = os.path.join(BASE_DIR, "Emotion + Posture Detector v5.0.ico") 
font_path = os.path.join(BASE_DIR, "ARIALBD 1.ttf")
# ==================================================

# === THAM S·ªê NG∆Ø·ª†NG THEO B·∫¢NG THAM CHI·∫æU DOCX ===
THRESHOLD_P_MAX_DEFAULT = 0.01  # Ng∆∞·ª°ng tin c·∫≠y chung (œÑ)
THRESHOLD_DELTA_TOP2_DEFAULT = 0.001 # Ng∆∞·ª°ng nh·∫≠p nh·∫±ng chung (Œ¥)
STABILITY_WINDOW_FRAMES = 1 # C·ª≠a s·ªï ·ªïn ƒë·ªãnh ng·∫Øn h·∫°n W (30 khung)
STABILITY_DOMINANCE_RATIO = 0.8 # Nh√£n ph·∫£i chi·∫øm t·ªëi thi·ªÉu 80% trong c·ª≠a s·ªï 30 frames ƒë·ªÉ ƒë∆∞·ª£c x√°c nh·∫≠n
BAD_POSTURE_WARNING_FRAMES = 600 # Ng∆∞·ª°ng c·∫£nh b√°o t∆∞ th·∫ø (kho·∫£ng 40 gi√¢y ·ªü 15 fps)

# Ng∆∞·ª°ng P_MAX v√† Delta TOP2 c·ª• th·ªÉ theo b·∫£ng DOCX
EMOTION_THRESHOLDS = {
    'Gi·∫≠n d·ªØ': {'p_max': 0.01, 'delta': 0.001}, # Gi·∫£m ng∆∞·ª°ng P_MAX xu·ªëng 1%
    'Gh√™ s·ª£': {'p_max': 0.01, 'delta': 0.001},
    'S·ª£ h√£i': {'p_max': 0.01, 'delta': 0.001},
    'Vui v·∫ª': {'p_max': 0.01, 'delta': 0.001}, 
    'Bu·ªìn': {'p_max': 0.01, 'delta': 0.001}, 
    'B·∫•t ng·ªù': {'p_max': 0.01, 'delta': 0.001},
    'Trung l·∫≠p': {'p_max': 0.01, 'delta': 0.001}
}
# === H·∫æT PH·∫¶N KHAI B√ÅO M·ªöI ===

# C·∫£m x√∫c b·∫•t l·ª£i: Bu·ªìn, Gi·∫≠n, S·ª£ h√£i
NEGATIVE_EMOTIONS = ['Bu·ªìn', 'Gi·∫≠n d·ªØ', 'S·ª£ h√£i'] 

# GLOBALS & KH·ªûI T·∫†O CHUNG
latest_frame = None
frame_lock = Lock()
loading_window = None
progress_bar = None
progress_label = None
flask_app = Flask(__name__)
is_running = False 
broadcast_thread = None
current_mode = 'camera' 
detection_thread = None 
thread_lock = Lock()

# C√ÅC THAY ƒê·ªîI M·ªöI V·ªÄ LOGGING D·ªÆ LI·ªÜU
SCAN_MIN_DURATION = 1800.0 # B·∫Øt bu·ªôc qu√©t t·ªëi thi·ªÉu 1800 gi√¢y ~ 30 ph√∫t
DATA_LOGS = []           # Danh s√°ch to√†n c·ª•c ƒë·ªÉ l∆∞u log d·ªØ li·ªáu
LOG_LOCK = Lock()        # Lock cho vi·ªác ghi/ƒë·ªçc DATA_LOGS

history = deque(maxlen=150) # ƒê∆∞a history v·ªÅ global ƒë·ªÉ truy c·∫≠p khi d·ª´ng
session_start_time = None
session_end_time = None
bad_posture_total_frames = 0
total_detection_frames = 0

# PH·∫¶N V·∫º KHUNG
ROI_BOX = None
ROI_DRAWING = False
ROI_ACTIVE = False
roi_start = None
roi_end = None
roi_status_text = "V·∫Ω khung ROI: T·∫ÆT"
roi_status_color = (255, 0, 0)
ROI_IMAGE_PATH = None
ROI_IMAGE_BUFFER = None
roi_emotion_label = None
DISPLAY_SCALE_X = 1.0
DISPLAY_SCALE_Y = 1.0

# FULLSCREEN
INCIDENT_STATE = None
INCIDENT_START_TIME = None
INCIDENT_START_TIME_STR = None

ABNORMAL_THRESHOLD = 6  # gi√¢y

ROI_LOGS = []
roi_scan_start_time = None

# G·ª¨I D·ªÆ LI·ªÜU L√äN WEB
ROI_ALERT_HISTORY = deque(maxlen=60)   # l∆∞u 60 gi√¢y

log_directory = BASE_DIR # Khai b√°o bi·∫øn to√†n c·ª•c cho th∆∞ m·ª•c log, m·∫∑c ƒë·ªãnh l√† BASE_DIR

force_exit_no_report = False
# UTILITIES CHUNG

def set_opencv_window_icon(window_title, icon_path):
    """
    ƒê·ªïi icon taskbar + title bar cho c·ª≠a s·ªï OpenCV (Windows only)
    """
    try:
        hwnd = win32gui.FindWindow(None, window_title)
        if not hwnd:
            return

        hicon = win32gui.LoadImage(
            None,
            icon_path,
            win32con.IMAGE_ICON,
            0,
            0,
            win32con.LR_LOADFROMFILE | win32con.LR_DEFAULTSIZE
        )

        # ICON nh·ªè (title bar)
        win32gui.SendMessage(hwnd, win32con.WM_SETICON, win32con.ICON_SMALL, hicon)
        # ICON l·ªõn (taskbar)
        win32gui.SendMessage(hwnd, win32con.WM_SETICON, win32con.ICON_BIG, hicon)

    except Exception as e:
        print("[ICON ERROR]", e)

def open_aismartmonitor():
    import os, sys, subprocess

    if getattr(sys, 'frozen', False):
        base_dir = os.path.dirname(sys.executable)
    else:
        base_dir = os.path.dirname(os.path.abspath(__file__))

    exe_path = os.path.join(
        base_dir,
        "AISmartMonitor",
        "AISmartMonitor.exe"
    )

    if not os.path.exists(exe_path):
        raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y AISmartMonitor.exe")

    subprocess.Popen([exe_path], cwd=os.path.dirname(exe_path))

def ask_student_id(parent):
    global ZONE_ID
    import tkinter as tk
    from tkinter import ttk

    result = {"ok": False}

    win = tk.Toplevel(parent)
    win.title("Nh·∫≠p m√£ h·ªçc sinh")
    win.resizable(False, False)
    win.attributes("-topmost", True)

    # ===== CƒÇN GI·ªÆA =====
    w, h = 360, 180
    x = (win.winfo_screenwidth() - w) // 2
    y = (win.winfo_screenheight() - h) // 2
    win.geometry(f"{w}x{h}+{x}+{y}")

    ttk.Label(
        win,
        text="Nh·∫≠p Student ID (Zone ID)",
        font=("Segoe UI", 11, "bold")
    ).pack(pady=(15, 5))

    entry = ttk.Entry(win, width=30, justify="center")
    entry.pack(pady=5)
    entry.focus()

    status_label = ttk.Label(win, text="", foreground="red")
    status_label.pack()

    btn_frame = ttk.Frame(win)
    btn_frame.pack(pady=15)

    def confirm(event=None):
        global ZONE_ID
        value = entry.get().strip()

        if not value:
            status_label.config(text="‚ö†Ô∏è Vui l√≤ng nh·∫≠p Student ID!")
            win.bell()
            return

        ZONE_ID = value
        result["ok"] = True
        win.destroy()

    def cancel():
        win.destroy()

    ttk.Button(btn_frame, text="X√°c nh·∫≠n", command=confirm).pack(side="left", padx=10)
    ttk.Button(btn_frame, text="H·ªßy", command=cancel).pack(side="left", padx=10)

    # ===== ENTER = X√ÅC NH·∫¨N =====
    entry.bind("<Return>", confirm)

    win.transient(parent)
    win.grab_set()
    parent.wait_window(win)

    return result["ok"]

def send_incident(state, start_time_str, duration):
    global ZONE_ID

    try:
        now = datetime.datetime.now()
        end_str = now.strftime('%H:%M:%S')
        date_str = now.strftime('%Y-%m-%d')

        payload = {
            "class_id": class_name,
            "zone_id": str(ZONE_ID),
            "issue_type": state,          # Emotion/Posture
            "start_time": start_time_str,
            "end_time": end_str,
            "duration_seconds": duration,
            "date": date_str,
            "scan_mode": SCAN_MODE
        }

        requests.post(SERVER_URL, json=payload, timeout=2)
        print("[INCIDENT SENT]", payload)

    except Exception as e:
        print("[ERROR] Send incident:", e)

def mouse_draw_roi(event, x, y, flags, param):
    global roi_start, roi_end, ROI_DRAWING, ROI_BOX
    global DISPLAY_SCALE_X, DISPLAY_SCALE_Y

    if ROI_ACTIVE:
        return

    if not ROI_DRAWING:
        return
    
    # CHUY·ªÇN T·ªåA ƒê·ªò CHU·ªòT ‚Üí FRAME G·ªêC
    fx = int(x * DISPLAY_SCALE_X)
    fy = int(y * DISPLAY_SCALE_Y)

    if event == cv2.EVENT_LBUTTONDOWN and ROI_DRAWING:
        roi_start = (fx, fy)
        roi_end = (fx, fy)

    elif event == cv2.EVENT_MOUSEMOVE and ROI_DRAWING and roi_start:
        roi_end = (fx, fy)

    elif event == cv2.EVENT_LBUTTONUP and ROI_DRAWING:
        roi_end = (fx, fy)

        x1, y1 = roi_start
        x2, y2 = roi_end

        ROI_BOX = (
            min(x1, x2),
            min(y1, y2),
            max(x1, x2),
            max(y1, y2)
        )

        roi_start = None
        roi_end = None

def mouse_draw_roi_fullscreen(event, x, y, flags, param):
    if ROI_ACTIVE:
        return

    global roi_start, roi_end, ROI_BOX, ROI_DRAWING
    global scale_factor

    if not ROI_DRAWING:
        return

    # QUY ƒê·ªîI T·ªåA ƒê·ªò T·ª™ ·∫¢NH HI·ªÇN TH·ªä ‚Üí FRAME G·ªêC
    fx = int(x / scale_factor)
    fy = int(y / scale_factor)

    if event == cv2.EVENT_LBUTTONDOWN:
        roi_start = (fx, fy)
        roi_end = None

    elif event == cv2.EVENT_MOUSEMOVE and roi_start is not None:
        roi_end = (fx, fy)

    elif event == cv2.EVENT_LBUTTONUP and roi_start is not None:
        roi_end = (fx, fy)
        x1, y1 = roi_start
        x2, y2 = roi_end

        ROI_BOX = (
            min(x1, x2),
            min(y1, y2),
            max(x1, x2),
            max(y1, y2)
        )
        roi_start = None
        roi_end = None

def show_success_with_open_folder(parent, export_file_path_csv, summary_path_txt):
    import tkinter as tk
    from tkinter import ttk
    import os, sys, subprocess
    import winsound

    winsound.MessageBeep(winsound.MB_ICONEXCLAMATION)

    folder_path = os.path.dirname(os.path.abspath(export_file_path_csv))

    csv_name = os.path.basename(export_file_path_csv)
    txt_name = os.path.basename(summary_path_txt)

    win = tk.Toplevel(parent)
    win.title("Th√†nh c√¥ng!")
    win.attributes("-topmost", True)
    win.resizable(False, False)

    # üìê K√≠ch th∆∞·ªõc CHU·∫®N ‚Äì kh√¥ng bao gi·ªù v·ª°
    w, h = 560, 260
    x = (win.winfo_screenwidth() - w) // 2
    y = (win.winfo_screenheight() - h) // 2
    win.geometry(f"{w}x{h}+{x}+{y}")

    frame = ttk.Frame(win)
    frame.pack(fill="both", expand=True, padx=20, pady=15)

    msg = (
        "‚úÖ ƒê√£ xu·∫•t b√°o c√°o th√†nh c√¥ng!\n\n"
        f"‚Ä¢ File chi ti·∫øt (CSV): {csv_name}\n"
        f"‚Ä¢ File t·ªïng h·ª£p (TXT): {txt_name}\n\n"
        "üìå D·ªØ li·ªáu phi√™n qu√©t hi·ªán t·∫°i ƒë√£ ƒë∆∞·ª£c X√ìA\n"
        "ƒë·ªÉ chu·∫©n b·ªã cho phi√™n m·ªõi."
    )

    label = ttk.Label(
        frame,
        text=msg,
        justify="left",
        wraplength=520
    )
    label.pack(anchor="w")

    # ====== N√öT ======
    btn_frame = ttk.Frame(frame)
    btn_frame.pack(pady=15)

    def open_folder():
        if sys.platform.startswith("win"):
            os.startfile(folder_path)
        elif sys.platform == "darwin":
            subprocess.Popen(["open", folder_path])
        else:
            subprocess.Popen(["xdg-open", folder_path])

    ttk.Button(btn_frame, text="üìÇ M·ªü th∆∞ m·ª•c", command=open_folder).pack(side="left", padx=12)
    ttk.Button(btn_frame, text="OK", command=win.destroy).pack(side="left", padx=12)

    win.focus_force()

def show_export_success_word(parent, folder_path):
    import tkinter as tk
    from tkinter import ttk
    import os, sys, subprocess, winsound

    winsound.MessageBeep(winsound.MB_ICONASTERISK)

    win = tk.Toplevel(parent)
    win.title("Xu·∫•t b√°o c√°o th√†nh c√¥ng")
    win.resizable(False, False)
    win.attributes("-topmost", True)

    # CƒÉn gi·ªØa
    w, h = 520, 200
    x = (win.winfo_screenwidth() - w) // 2
    y = (win.winfo_screenheight() - h) // 2
    win.geometry(f"{w}x{h}+{x}+{y}")

    msg = (
        "‚úÖ ƒê√£ xu·∫•t b√°o c√°o ROI th√†nh c√¥ng!\n\n"
        "üìÅ Th∆∞ m·ª•c l∆∞u b√°o c√°o:\n"
        f"{folder_path}"
    )

    label = ttk.Label(win, text=msg, wraplength=480, justify="left")
    label.pack(padx=15, pady=15)

    btn_frame = ttk.Frame(win)
    btn_frame.pack(pady=10)

    def open_folder():
        if sys.platform.startswith("win"):
            os.startfile(folder_path)
        elif sys.platform == "darwin":
            subprocess.Popen(["open", folder_path])
        else:
            subprocess.Popen(["xdg-open", folder_path])

    ttk.Button(btn_frame, text="üìÇ M·ªü th∆∞ m·ª•c", command=open_folder).pack(side="left", padx=10)
    ttk.Button(btn_frame, text="OK", command=win.destroy).pack(side="left", padx=10)

    win.focus_force()

def export_roi_to_word():
    global ROI_LOGS, ROI_BOX, log_directory, ROI_IMAGE_BUFFER, ZONE_ID
    from io import BytesIO
    from docx import Document
    from docx.shared import Inches
    from collections import Counter
    import datetime, os
    from PIL import Image
    import matplotlib.pyplot as plt

    if not ROI_LOGS or not ROI_BOX:
        return

    os.makedirs(log_directory, exist_ok=True)

    doc = Document()

    # ===== TI√äU ƒê·ªÄ =====
    doc.add_heading(
        f"B√ÅO C√ÅO PH√ÇN T√çCH ROI - EMOTION & POSTURE - HS-{ZONE_ID}", level=1
    )

    # ===== TH√îNG TIN CHUNG =====
    now = datetime.datetime.now()
    doc.add_paragraph(f"Th·ªùi gian xu·∫•t b√°o c√°o: {now.strftime('%d/%m/%Y %H:%M:%S')}")

    start_time = ROI_LOGS[0]['time']
    end_time = ROI_LOGS[-1]['time']
    total_duration = int(end_time - start_time)
    total_frames = len(ROI_LOGS)

    doc.add_paragraph(f"T·ªïng th·ªùi gian qu√©t ROI: {total_duration} gi√¢y (~{total_duration/60:.2f} ph√∫t)")
    doc.add_paragraph(f"T·ªïng s·ªë frame ghi nh·∫≠n: {total_frames} frame")

    # ===== H√åNH ·∫¢NH ROI (GI·ªÆ NGUY√äN) =====
    if ROI_IMAGE_BUFFER:
        doc.add_heading("H√¨nh ·∫£nh v√πng ROI", level=2)

        image_stream = BytesIO(ROI_IMAGE_BUFFER)
        img = Image.open(image_stream)

        max_width_inch = 6
        max_height_inch = 8
        dpi = 96

        max_width_px = int(max_width_inch * dpi)
        max_height_px = int(max_height_inch * dpi)

        img.thumbnail((max_width_px, max_height_px), Image.Resampling.LANCZOS)

        img_stream_resized = BytesIO()
        img.save(img_stream_resized, format="PNG")
        img_stream_resized.seek(0)

        doc.add_picture(
            img_stream_resized,
            width=Inches(img.width / dpi),
            height=Inches(img.height / dpi)
        )

    # ================== TH·ªêNG K√ä & PH√ÇN T√çCH ==================

    emo_counter = Counter([log['emotion'] for log in ROI_LOGS])
    posture_list = [log.get('posture') for log in ROI_LOGS if log.get('posture')]
    pos_counter = Counter(posture_list)

    # ===== TH·ªêNG K√ä C·∫¢M X√öC (%) =====
    doc.add_heading("Th·ªëng k√™ bi·ªÉu c·∫£m khu√¥n m·∫∑t (%)", level=2)

    emo_percent = {}
    for emo, count in emo_counter.items():
        pct = round(count / total_frames * 100, 2)
        emo_percent[emo] = pct
        doc.add_paragraph(f"- {emo}: {pct}%")

    # ===== BI·ªÇU ƒê·ªí C·∫¢M X√öC =====
    if emo_percent:
        plt.figure(figsize=(6, 4))
        plt.bar(emo_percent.keys(), emo_percent.values())
        plt.title("Ph√¢n b·ªë bi·ªÉu c·∫£m khu√¥n m·∫∑t")
        plt.ylabel("T·ª∑ l·ªá (%)")
        plt.xticks(rotation=30)
        plt.tight_layout()

        emo_chart = BytesIO()
        plt.savefig(emo_chart, format="PNG")
        plt.close()
        emo_chart.seek(0)

        doc.add_picture(emo_chart, width=Inches(5))

    # ===== TH·ªêNG K√ä T∆Ø TH·∫æ (%) =====
    if pos_counter:
        doc.add_heading("Th·ªëng k√™ t∆∞ th·∫ø (%)", level=2)

        pos_percent = {}
        for pos, count in pos_counter.items():
            pct = round(count / total_frames * 100, 2)
            pos_percent[pos] = pct
            doc.add_paragraph(f"- {pos}: {pct}%")

        # ===== BI·ªÇU ƒê·ªí T∆Ø TH·∫æ =====
        plt.figure(figsize=(6, 4))
        plt.bar(pos_percent.keys(), pos_percent.values())
        plt.title("Ph√¢n b·ªë t∆∞ th·∫ø")
        plt.ylabel("T·ª∑ l·ªá (%)")
        plt.xticks(rotation=30)
        plt.tight_layout()

        pos_chart = BytesIO()
        plt.savefig(pos_chart, format="PNG")
        plt.close()
        pos_chart.seek(0)

        doc.add_picture(pos_chart, width=Inches(5))

    # ================== T∆Ø V·∫§N S·ª®C KH·ªéE H·ªåC ƒê∆Ø·ªúNG ==================

    NEGATIVE_EMOTIONS = ['Bu·ªìn', 'Gi·∫≠n d·ªØ', 'S·ª£ h√£i', 'Gh√™ s·ª£']

    negative_emo_ratio = sum(
        emo_percent.get(emo, 0) for emo in NEGATIVE_EMOTIONS
    )

    bad_posture_ratio = pos_percent.get('C√∫i nhi·ªÅu (Bad)', 0)
    posture_coverage = sum(pos_percent.values())

    summary_signal_emo = "XANH"
    summary_signal_pos = "XANH"
    quality_check = "T·ªêT"

    if negative_emo_ratio >= 40:
        summary_signal_emo = "V√ÄNG (B·∫•t l·ª£i ‚â• 40%)"

    if posture_coverage < 50:
        quality_check = "C·∫¶N C·∫¢I THI·ªÜN"
        summary_signal_pos = "V√ÄNG (Bao ph·ªß < 50%)"
    elif bad_posture_ratio >= 5:
        summary_signal_pos = "V√ÄNG (C√∫i nhi·ªÅu ‚â• 5%)"

    overall_signal = "XANH üü¢"
    if "V√ÄNG" in summary_signal_emo or "V√ÄNG" in summary_signal_pos:
        overall_signal = "V√ÄNG üü°"
        if "V√ÄNG" in summary_signal_emo and "V√ÄNG" in summary_signal_pos:
            overall_signal = "ƒê·ªé (Nguy c∆° k√©p) üî¥"

    doc.add_heading("T√≠n hi·ªáu c·∫£nh b√°o t·ªïng h·ª£p", level=2)
    doc.add_paragraph(f"- T√≠n hi·ªáu bi·ªÉu c·∫£m khu√¥n m·∫∑t: {summary_signal_emo}")
    doc.add_paragraph(f"- T√≠n hi·ªáu t∆∞ th·∫ø: {summary_signal_pos}")
    doc.add_paragraph(f"- Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu t∆∞ th·∫ø: {quality_check}")

    doc.add_heading("ƒê√°nh gi√° & t∆∞ v·∫•n s·ª©c kh·ªèe h·ªçc ƒë∆∞·ªùng", level=2)


    doc.add_paragraph(f"M·ª©c ƒë·ªô nguy c∆° t·ªïng h·ª£p (phi√™n qu√©t): {overall_signal}")

    if overall_signal == "XANH üü¢":
        doc.add_paragraph(
            "T·ªîNG H·ª¢P: Ng∆∞·ª°ng an to√†n.\n"
            "KHUY·∫æN NGH·ªä: Duy tr√¨ theo d√µi ƒë·ªãnh k·ª≥. "
            "Gi√°o vi√™n c√≥ th·ªÉ nh·∫Øc nh·ªü ƒëi·ªÅu ch·ªânh t∆∞ th·∫ø ho·∫∑c thay ƒë·ªïi ho·∫°t ƒë·ªông nh·∫π khi c·∫ßn."
        )

    elif overall_signal == "V√ÄNG üü°":
        doc.add_paragraph(
            "T·ªîNG H·ª¢P: Nguy c∆° trung b√¨nh, c·∫ßn s√†ng l·ªçc nhanh.\n"
            "QUY TR√åNH ƒê·ªÄ XU·∫§T:\n"
            "‚Ä¢ Quan s√°t b·ªï sung trong c√°c bu·ªïi h·ªçc ti·∫øp theo.\n"
            "‚Ä¢ Nh·∫Øc nh·ªü ƒëi·ªÅu ch·ªânh t∆∞ th·∫ø, thay ƒë·ªïi ho·∫°t ƒë·ªông.\n"
            "‚Ä¢ Trao ƒë·ªïi nh·∫π nh√†ng nh·∫±m gi·∫£m cƒÉng th·∫≥ng t√¢m l√Ω."
        )

    elif overall_signal == "ƒê·ªé (Nguy c∆° k√©p) üî¥":
        doc.add_paragraph(
            "T·ªîNG H·ª¢P: Nguy c∆° cao, c·∫ßn k√≠ch ho·∫°t t∆∞ v·∫•n c√° nh√¢n.\n"
            "ƒê·ªÄ XU·∫§T:\n"
            "‚Ä¢ Ki·ªÉm ch·ª©ng d·ªØ li·ªáu k·ªπ thu·∫≠t v√† quan s√°t tr·ª±c ti·∫øp.\n"
            "‚Ä¢ Tham v·∫•n gi√°o vi√™n ch·ªß nhi·ªám v√† chuy√™n vi√™n t√¢m l√Ω.\n"
            "‚Ä¢ X√¢y d·ª±ng k·∫ø ho·∫°ch h·ªó tr·ª£ c√° nh√¢n h√≥a cho h·ªçc sinh."
        )

    # ===== L∆ØU FILE =====
    filename = f"ROI_Report_{now.strftime('%Y%m%d_%H%M%S')}.docx"
    filepath = os.path.join(log_directory, filename)
    doc.save(filepath)

    print(f"[INFO] ƒê√£ l∆∞u b√°o c√°o ROI t·∫°i: {filepath}")
    show_export_success_word(root, log_directory)

def analyze_and_export_csv():
    """
    Ph√¢n t√≠ch d·ªØ li·ªáu, m·ªü h·ªôp tho·∫°i ch·ªçn n∆°i l∆∞u, xu·∫•t file CSV v√† TXT c√πng v·ªã tr√≠, 
    kh·∫Øc ph·ª•c l·ªói font CSV, v√† reset d·ªØ li·ªáu.
    """
    global DATA_LOGS, history, session_start_time, total_detection_frames, bad_posture_total_frames, BASE_DIR
    from datetime import datetime
    # 0. Ki·ªÉm tra d·ªØ li·ªáu h·ª£p l·ªá (Gi·ªØ nguy√™n ki·ªÉm tra)
    if len(DATA_LOGS) < 2 or total_detection_frames == 0:
        return messagebox.showwarning("Th√¥ng b√°o", "Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch (c·∫ßn >1 b·∫£n ghi ho·∫∑c Frames > 0).")

    # X√°c ƒë·ªãnh th·ªùi gian phi√™n
    session_end_time = DATA_LOGS[-1]['timestamp']
    session_duration = session_end_time - session_start_time
    
    # --- A. X·ª¨ L√ù ƒê∆Ø·ªúNG D·∫™N L∆ØU FILE (Th·ª±c hi·ªán h·ªèi ch·ªó l∆∞u) ---
    now = datetime.now()
    date_time_string = now.strftime("%H:%M:%S - Ng√†y %d/%m/%Y")
    timestamp_str = now.strftime("%Y%m%d_%H%M%S") 
    file_name_base_csv = f"Emotion_Posture_Report_{timestamp_str}.csv"
    file_name_base = f"Emotion_Posture_Report_{timestamp_str}"
    
    # G√°n ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh ho·∫∑c ƒë∆∞·ªùng d·∫´n ng∆∞·ªùi ƒë√£ thay ƒë·ªïi ƒë·ªÉ l∆∞u file
    export_file_path_csv = os.path.join(log_directory, file_name_base_csv)
        
    # T·∫°o ƒë∆∞·ªùng d·∫´n cho file B√ÅO C√ÅO T·ªîNG H·ª¢P (.txt) c√πng th∆∞ m·ª•c v·ªõi CSV
    base_dir_csv = os.path.dirname(export_file_path_csv)
    summary_path_txt = os.path.join(base_dir_csv, f"{file_name_base}_SUMMARY.txt")
    
    
    # --- B. PH√ÇN T√çCH V√Ä T√çNH TO√ÅN ---
    # *ƒê√¢y l√† ph·∫ßn ph·ª©c t·∫°p nh·∫•t, t√¥i gi·ªØ l·∫°i logic ph√¢n t√≠ch t·ªïng h·ª£p t·ª´ c√°c l·∫ßn tr∆∞·ªõc ƒë·ªÉ t·∫°o ra file TXT*
    
    emotion_duration = {}
    posture_duration = {}
    for i in range(len(DATA_LOGS) - 1):
        current = DATA_LOGS[i]
        next_record = DATA_LOGS[i+1]
        duration = next_record['timestamp'] - current['timestamp']
        emotion = current.get('emotion', 'Unknown')
        emotion_duration[emotion] = emotion_duration.get(emotion, 0) + duration
        posture = current.get('posture_status', 'N/A')
        posture_duration[posture] = posture_duration.get(posture, 0) + duration

    # T√≠nh T·ª∑ l·ªá
    total_valid_emo_duration = sum(dur for emo, dur in emotion_duration.items() if emo not in ['Unknown', 'N/A'])
    total_posture_duration = sum(posture_duration.values())
    emo_ratios = {emo: (dur / total_valid_emo_duration) * 100 for emo, dur in emotion_duration.items()} if total_valid_emo_duration > 0 else {}
    pos_ratios = {pos: (dur / total_posture_duration) * 100 for pos, dur in posture_duration.items()} if total_posture_duration > 0 else {}
    
    NEGATIVE_EMOTIONS = ['Bu·ªìn', 'Gi·∫≠n d·ªØ', 'S·ª£ h√£i', 'Gh√™ s·ª£'] 
    negative_emo_ratio = sum(emo_ratios.get(emo, 0) for emo in NEGATIVE_EMOTIONS)
    no_posture_duration = posture_duration.get('N/A', 0) + posture_duration.get('Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø', 0)
    no_posture_ratio = (no_posture_duration / total_posture_duration) * 100 if total_posture_duration > 0 else 0
    bad_posture_ratio = pos_ratios.get('C√∫i nhi·ªÅu (Bad)', 0)
    posture_coverage = 100 - no_posture_ratio
    fps = total_detection_frames / session_duration if session_duration > 0 else 0
    
    # Quy ƒë·ªïi t√≠n hi·ªáu
    summary_signal_emo = 'XANH'
    summary_signal_pos = 'XANH'
    quality_check = "T·ªêT"
    if negative_emo_ratio >= 40: summary_signal_emo = 'V√ÄNG (B·∫•t l·ª£i >= 40%)'
    if posture_coverage < 50:
        quality_check = "C·∫¶N C·∫¢I THI·ªÜN"
        summary_signal_pos = 'V√ÄNG (Bao ph·ªß < 50%)'
    elif bad_posture_ratio >= 5: summary_signal_pos = 'V√ÄNG (C√∫i nhi·ªÅu >= 5%)'
    
    # --- LOGIC X√ÅC ƒê·ªäNH NG∆Ø·ª†NG T∆Ø V·∫§N (T·ªîNG H·ª¢P - B·ªî SUNG) ---
    overall_signal = 'XANH üü¢'
    if 'V√ÄNG' in summary_signal_emo or 'V√ÄNG' in summary_signal_pos:
        overall_signal = 'V√ÄNG üü°'
        # Tr∆∞·ªùng h·ª£p r·ªßi ro k√©p (c·∫£ hai k√™nh ƒë·ªÅu V√ÄNG), xem nh∆∞ ƒê·ªé (k√≠ch ho·∫°t t∆∞ v·∫•n c√° nh√¢n) trong b·ªëi c·∫£nh b√°o c√°o 1 l·∫ßn
        if 'V√ÄNG' in summary_signal_emo and 'V√ÄNG' in summary_signal_pos:
            overall_signal = 'ƒê·ªé (Nguy c∆° k√©p) üî¥'

    consultation_recommendation = ""

    if overall_signal == 'XANH üü¢':
        consultation_recommendation = """
    T·ªîNG H·ª¢P: Ng∆∞·ª°ng an to√†n.
    KHUY·∫æN NGH·ªä: Duy tr√¨ theo d√µi ƒë·ªãnh k·ª≥.
    Gi√°o vi√™n c√≥ th·ªÉ nh·∫Øc nh·ªü ƒëi·ªÅu ch·ªânh t∆∞ th·∫ø ho·∫∑c thay ƒë·ªïi ho·∫°t ƒë·ªông nh·∫π khi c·∫ßn.
    """
    elif overall_signal == 'V√ÄNG üü°':
        consultation_recommendation = """
    T·ªîNG H·ª¢P: Nguy c∆° trung b√¨nh, c·∫ßn s√†ng l·ªçc nhanh.
    QUY TR√åNH ƒê·ªÄ XU·∫§T:
    ‚Ä¢ Quan s√°t b·ªï sung trong c√°c bu·ªïi h·ªçc ti·∫øp theo.
    ‚Ä¢ Nh·∫Øc nh·ªü ƒëi·ªÅu ch·ªânh t∆∞ th·∫ø, thay ƒë·ªïi ho·∫°t ƒë·ªông.
    ‚Ä¢ Trao ƒë·ªïi nh·∫π nh√†ng nh·∫±m gi·∫£m cƒÉng th·∫≥ng t√¢m l√Ω.
    """
    elif overall_signal == 'ƒê·ªé (Nguy c∆° k√©p) üî¥':
        consultation_recommendation = """
    T·ªîNG H·ª¢P: Nguy c∆° cao, c·∫ßn k√≠ch ho·∫°t t∆∞ v·∫•n c√° nh√¢n.
    ƒê·ªÄ XU·∫§T:
    ‚Ä¢ Ki·ªÉm ch·ª©ng d·ªØ li·ªáu k·ªπ thu·∫≠t v√† quan s√°t tr·ª±c ti·∫øp.
    ‚Ä¢ Tham v·∫•n gi√°o vi√™n ch·ªß nhi·ªám v√† chuy√™n vi√™n t√¢m l√Ω.
    ‚Ä¢ X√¢y d·ª±ng k·∫ø ho·∫°ch h·ªó tr·ª£ c√° nh√¢n h√≥a cho h·ªçc sinh.
    """

    # --- C. T·∫†O N·ªòI DUNG B√ÅO C√ÅO T·ªîNG H·ª¢P (TXT) ---
    report_content = f"""
===================================================
| B√ÅO C√ÅO T·ªîNG H·ª¢P PH√ÇN T√çCH (M·∫´u 01a/01b)
| TH·ªúI ƒêI·ªÇM XU·∫§T: {date_time_string}
===================================================
1. TH√îNG TIN CHUNG
- T·ªïng th·ªùi gian qu√©t: {session_duration:.2f} gi√¢y (~{session_duration/60:.2f} ph√∫t)
- T·ªïng Frames qu√©t: {total_detection_frames}
- T·ªëc ƒë·ªô khung h√¨nh (FPS): {fps:.2f} FPS

2. PH√ÇN T√çCH T∆Ø TH·∫æ (POSTURE)
- T·ª∑ l·ªá Bao ph·ªß (Posture Coverage): {posture_coverage:.2f}% (M·ª•c ti√™u: > 60%)
- Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu T∆∞ th·∫ø: {quality_check}
--- T·ª∑ l·ªá Chi ti·∫øt ---
- Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø: {no_posture_ratio:.2f}%
- Ng·ªìi th·∫≥ng (T·ªët): {pos_ratios.get('Ng·ªìi th·∫≥ng (Good)', 0):.2f}%
- H∆°i c√∫i (C·∫£nh b√°o): {pos_ratios.get('H∆°i c√∫i (Warning)', 0):.2f}%
- **C√∫i nhi·ªÅu (Bad): {bad_posture_ratio:.2f}%** (Ng∆∞·ª°ng M·∫´u 01b: >= 5%)

3. PH√ÇN T√çCH BI·ªÇU C·∫¢M KHU√îN M·∫∂T
- T·ª∑ l·ªá Bi·ªÉu c·∫£m khu√¥n m·∫∑t B·∫•t l·ª£i: **{negative_emo_ratio:.2f}%** (Ng∆∞·ª°ng M·∫´u 01b: >= 40%)
--- T·ª∑ l·ªá Chi ti·∫øt ---
- Bu·ªìn: {emo_ratios.get('Bu·ªìn', 0):.2f}%
- Gi·∫≠n d·ªØ: {emo_ratios.get('Gi·∫≠n d·ªØ', 0):.2f}%
- S·ª£ h√£i: {emo_ratios.get('S·ª£ h√£i', 0):.2f}%
- Vui v·∫ª: {emo_ratios.get('Vui v·∫ª', 0):.2f}%
- Trung l·∫≠p: {emo_ratios.get('Trung l·∫≠p', 0):.2f}%
- B·∫•t ng·ªù: {emo_ratios.get('B·∫•t ng·ªù', 0):.2f}%
- Gh√™ s·ª£: {emo_ratios.get('Gh√™ s·ª£', 0):.2f}%

4. T√çN HI·ªÜU C·∫¢NH B√ÅO T·ªîNG H·ª¢P L·ªöP (M·∫´u 01a)
- T√≠n hi·ªáu Bi·ªÉu c·∫£m khu√¥n m·∫∑t: {summary_signal_emo}
- T√≠n hi·ªáu T∆∞ th·∫ø: {summary_signal_pos}

===================================================
5. T∆Ø V·∫§N V√Ä KHUY·∫æN NGH·ªä D·ª∞A TR√äN NG∆Ø·ª†NG
- M·ª©c ƒë·ªô Nguy c∆° T·ªïng h·ª£p (Phi√™n qu√©t): {overall_signal}
{consultation_recommendation}
===================================================
"""
    
    # --- D. T·∫†O N·ªòI DUNG CSV (Chi ti·∫øt) ---
    data_to_export = [
        ["T·ªïng th·ªùi gian qu√©t", f"{session_duration:.2f} gi√¢y"],
        ["---", "---"],
        ["Ph√¢n t√≠ch Bi·ªÉu c·∫£m", "Th·ªùi gian (gi√¢y)"],
    ]
    for emo, dur in emotion_duration.items():
        data_to_export.append([emo, f"{dur:.2f}"])
        
    data_to_export.extend([
        ["---", "---"],
        ["Ph√¢n t√≠ch T∆∞ th·∫ø (T·ªïng h·ª£p)", "Th·ªùi gian (gi√¢y)"],
    ])
    for pos, dur in posture_duration.items():
        data_to_export.append([pos, f"{dur:.2f}"])
        
    # 5. XU·∫§T FILE (CSV v√† TXT)
    try:
        # Xu·∫•t file CSV: D√πng encoding='utf-8-sig' ƒë·ªÉ S·ª¨A L·ªñI CH·ªÆ TI·∫æNG VI·ªÜT
        with open(export_file_path_csv, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.writer(csvfile) 
            writer.writerows(data_to_export) 
        
        # Xu·∫•t file B√ÅO C√ÅO T·ªîNG H·ª¢P (.txt)
        with open(summary_path_txt, 'w', encoding='utf-8') as f:
            f.write(report_content)

        show_success_with_open_folder(
            root,# C·ª≠a s·ªï ch√≠nh Tk
            export_file_path_csv,
            summary_path_txt
        )


    except Exception as e:
        messagebox.showerror("L·ªói Xu·∫•t File", f"Kh√¥ng th·ªÉ xu·∫•t file b√°o c√°o:\n{e}")

    # 6. X√≥a d·ªØ li·ªáu v√† reset bi·∫øn sau khi xu·∫•t
    DATA_LOGS.clear()
    history.clear()
    session_start_time = 0.0
    total_detection_frames = 0
    bad_posture_total_frames = 0

def set_log_directory():
    """M·ªü h·ªôp tho·∫°i ƒë·ªÉ ch·ªçn th∆∞ m·ª•c l∆∞u file log v√† c·∫≠p nh·∫≠t bi·∫øn to√†n c·ª•c."""
    global log_directory, root
    
    # L·∫•y ƒë∆∞·ªùng d·∫´n hi·ªán t·∫°i l√†m th∆∞ m·ª•c ban ƒë·∫ßu
    initial_dir = log_directory if os.path.isdir(log_directory) else os.path.expanduser("~")
    
    new_dir = filedialog.askdirectory(
        parent=root,
        initialdir=initial_dir,
        title="Ch·ªçn th∆∞ m·ª•c l∆∞u File Log"
    )
    
    if new_dir:
        log_directory = new_dir
        # Hi·ªÉn th·ªã th√¥ng b√°o
        messagebox.showinfo("Th√†nh c√¥ng", f"Th∆∞ m·ª•c l∆∞u log ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√†nh c√¥ng:\n{log_directory}")

def get_local_ip():
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect(('8.8.8.8', 80))
        ip = s.getsockname()[0]
    except Exception:
        ip = '127.0.0.1'
    finally:
        s.close()
    return ip

def udp_broadcast(message, port=5000, interval=5):
    """G·ª≠i broadcast UDP li√™n t·ª•c ƒë·ªÉ thi·∫øt b·ªã trong LAN b·∫Øt ƒë∆∞·ª£c link."""
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
    sock.settimeout(0.2)
    while True:
        try:
            sock.sendto(message.encode("utf-8"), ("<broadcast>", port))
        except Exception as e:
            print("Broadcast error:", e)
        time.sleep(interval)

def list_cameras():
    """Li·ªát k√™ camera s·ª≠ d·ª•ng pygrabber.dshow_graph."""
    graph = FilterGraph()
    devices = graph.get_input_devices()
    return devices

def draw_text_with_outline(draw, pos, text, font, text_color,
                           outline_color=(0, 0, 0), outline_width=1):
    x, y = pos
    for dx in range(-outline_width, outline_width + 1):
        for dy in range(-outline_width, outline_width + 1):
            if dx == 0 and dy == 0:
                continue
            draw.text((x + dx, y + dy), text, font=font, fill=outline_color)
    draw.text((x, y), text, font=font, fill=text_color)

def draw_filled_rectangle_with_outline(img, pt1, pt2, color,
                                      outline_color=(0, 0, 0),
                                      outline_width=1):
    cv2.rectangle(img,
                  (pt1[0] - outline_width, pt1[1] - outline_width),
                  (pt2[0] + outline_width, pt2[1] + outline_width),
                  outline_color, -1)
    cv2.rectangle(img, pt1, pt2, color, -1)

def calculate_angle(a, b, c):
    import math
    ax, ay = a
    bx, by = b
    cx, cy = c
    angle = math.degrees(
        math.atan2(cy - by, cx - bx) - math.atan2(ay - by, ax - bx)
    )
    return abs(angle)

def bring_window_to_front(window_name):
    """Set always on top + nh·∫£y ra tr∆∞·ªõc."""
    hwnd = win32gui.FindWindow(None, window_name)
    if hwnd:
        win32gui.SetWindowPos(hwnd, win32con.HWND_TOPMOST,
                              0, 0, 0, 0,
                              win32con.SWP_NOMOVE | win32con.SWP_NOSIZE)
        win32gui.ShowWindow(hwnd, win32con.SW_SHOWNORMAL)
        try:
            win32gui.SetForegroundWindow(hwnd)
        except Exception:
            pass

def show_warning(msg):
    """Hi·ªán c·∫£nh b√°o (lu√¥n hi·ªán tr√™n c√πng)."""
    win = tk.Toplevel()
    win.withdraw()
    win.attributes('-topmost', True)
    messagebox.showwarning("C·∫£nh b√°o", msg, parent=win)
    win.destroy()

# UTILITIES CHO TKINTER & LOADING

def generate_qr_code(link):
    """T·∫°o m√£ QR t·ª´ link v√† tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng ƒë·ªëi t∆∞·ª£ng PIL Image."""
    qr = qrcode.QRCode(
        version=1,
        error_correction=qrcode.constants.ERROR_CORRECT_L,
        box_size=10, # TƒÉng k√≠ch th∆∞·ªõc box_size
        border=4,
    )
    qr.add_data(link)
    qr.make(fit=True)
    # Tr·∫£ v·ªÅ ·∫£nh PIL Image, kh√¥ng ph·∫£i PhotoImage
    img_qr_pil = qr.make_image(fill_color="black", back_color="white").convert('RGB')
    return img_qr_pil

def copy_link_to_clipboard(link, link_window):
    global root
    root.clipboard_clear()
    root.clipboard_append(link)
    messagebox.showinfo("Th√¥ng b√°o", "ƒê√£ sao ch√©p ƒë∆∞·ªùng link v√†o Clipboard!")
    link_window.destroy() 

def copy_qr_to_clipboard(qr_image_pil, link_window):
    """
    Sao ch√©p ·∫£nh PIL Image (QR Code) v√†o Clipboard d∆∞·ªõi d·∫°ng DIB (Bitmap).
    CH·ªà ho·∫°t ƒë·ªông tr√™n Windows v√¨ s·ª≠ d·ª•ng win32clipboard.
    """
    try:
        # Chuy·ªÉn ƒë·ªïi PIL Image sang ƒë·ªãnh d·∫°ng BMP byte stream
        output = io.BytesIO()
        qr_image_pil.save(output, 'BMP')
        data = output.getvalue()[14:] # B·ªè qua BMP file header (14 bytes)
        
        # M·ªü Clipboard v√† ƒë·∫∑t d·ªØ li·ªáu
        win32clipboard.OpenClipboard()
        win32clipboard.EmptyClipboard()
        # ƒê·∫∑t format CF_DIB (Device Independent Bitmap)
        win32clipboard.SetClipboardData(win32con.CF_DIB, data)
        win32clipboard.CloseClipboard()
        
        messagebox.showinfo("Th√¥ng b√°o", "ƒê√£ sao ch√©p ·∫£nh QR Code v√†o Clipboard th√†nh c√¥ng!")
        link_window.destroy()
        
    except Exception as e:
        messagebox.showerror("L·ªói Sao Ch√©p ·∫¢nh", f"Kh√¥ng th·ªÉ sao ch√©p ·∫£nh QR Code v√†o Clipboard (Ch·ªâ h·ªó tr·ª£ Windows).\nL·ªói: {e}")

def show_stream_link(link):
    """Hi·ªÉn th·ªã ƒë∆∞·ªùng link Stream, M√£ QR v√† c√°c n√∫t h√†nh ƒë·ªông."""
    global root
    
    # 1. T·∫°o ·∫£nh QR d∆∞·ªõi d·∫°ng PIL Image
    qr_image_pil = generate_qr_code(link)
    
    # 2. Chuy·ªÉn ƒë·ªïi sang PhotoImage ƒë·ªÉ hi·ªÉn th·ªã trong Tkinter
    bio = io.BytesIO()
    qr_image_pil.save(bio, format='PNG')
    qr_photo = tk.PhotoImage(data=bio.getvalue())

    # 3. T·∫°o c·ª≠a s·ªï Toplevel
    link_window = tk.Toplevel(root)
    link_window.title("ƒê∆∞·ªùng Link Stream v√† M√£ QR")
    link_window.update() 

    # 4. Hi·ªÉn th·ªã M√£ QR
    qr_label = tk.Label(link_window, image=qr_photo)
    qr_label.image = qr_photo # Gi·ªØ tham chi·∫øu ƒë·ªÉ tr√°nh b·ªã Garbage Collection
    qr_label.pack(pady=10, padx=20)
    
    # 5. Hi·ªÉn th·ªã Text Label v√† Entry
    tk.Label(link_window, text="Qu√©t M√£ QR ho·∫∑c truy c·∫≠p ƒë∆∞·ªùng link sau:", 
             font=("Arial", 10, "bold")).pack(pady=(0, 5), padx=20)
    
    link_entry = tk.Entry(link_window, width=50, justify='center')
    link_entry.insert(0, link)
    link_entry.config(state="readonly")
    link_entry.pack(pady=5, padx=20)
    
    # 6. Khung ch·ª©a c√°c n√∫t
    button_frame = tk.Frame(link_window)
    button_frame.pack(pady=15)
    
    # N√∫t 1: Sao ch√©p Link
    copy_link_btn = tk.Button(button_frame,
                         text="Sao ch√©p Link",
                         command=lambda: copy_link_to_clipboard(link, link_window),
                         bg="#007ACC", fg="white", font=("Arial", 10, "bold"))
    copy_link_btn.pack(side=tk.LEFT, padx=5)

    # N√∫t 2: Sao ch√©p ·∫¢nh QR v√†o Clipboard (M·ªõi)
    copy_qr_btn = tk.Button(button_frame,
                         text="üìã Sao ch√©p ·∫¢nh QR",
                         command=lambda: copy_qr_to_clipboard(qr_image_pil, link_window),
                         bg="#FF9800", fg="white", font=("Arial", 10, "bold"))
    copy_qr_btn.pack(side=tk.LEFT, padx=5)

    # N√∫t 3: ƒê√≥ng
    close_btn = tk.Button(button_frame,
                         text="ƒê√≥ng",
                         command=link_window.destroy,
                         bg="#F44336", fg="white", font=("Arial", 10, "bold"))
    close_btn.pack(side=tk.LEFT, padx=5)
    
    # 7. CƒÉn gi·ªØa c·ª≠a s·ªï m·ªõi
    root.update_idletasks()
    link_window.update_idletasks()
    x = root.winfo_x() + (root.winfo_width() - link_window.winfo_reqwidth()) // 2
    y = root.winfo_y() + (root.winfo_height() - link_window.winfo_reqheight()) // 2
    link_window.geometry(f"+{x}+{y}")

def show_loading_window(title="ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng..."):
    global loading_window, progress_bar, progress_label, root

    loading_window = tk.Toplevel(root)
    loading_window.title(title)
    loading_window.geometry("400x140")
    loading_window.resizable(False, False)
#    loading_window.attributes('-topmost', True)

    tk.Label(loading_window, text="ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng, vui l√≤ng ch·ªù...",
             font=("Arial", 10)).pack(pady=10)

    progress_bar = ttk.Progressbar(loading_window, orient="horizontal", length=350, mode="determinate")
    progress_bar.pack(pady=10)
    progress_bar["maximum"] = 100
    progress_bar["value"] = 0

    progress_label = tk.Label(loading_window, text="0%", font=("Arial", 10, "bold"))
    progress_label.pack()

    # CƒÉn gi·ªØa c·ª≠a s·ªï loading
    root.update_idletasks()
    loading_window.update_idletasks()
    x = root.winfo_x() + (root.winfo_width() - loading_window.winfo_reqwidth()) // 2
    y = root.winfo_y() + (root.winfo_height() - loading_window.winfo_reqheight()) // 2
    loading_window.geometry(f"+{x}+{y}")

def update_progress(percent, text=None):
    if progress_bar and progress_label and loading_window and loading_window.winfo_exists():
        progress_bar["value"] = percent
        if text:
            progress_label.config(text=f"{text} ({percent}%)")
        else:
            progress_label.config(text=f"{percent}%")
        loading_window.update_idletasks()

def destroy_loading_window():
    global loading_window
    if loading_window and loading_window.winfo_exists():
        loading_window.destroy()

# FLASK STREAMING SETUP

def start_flask_server():
    global flask_app
    # S·ª¨A L·ªñI: D√πng c·ªïng ƒë√£ thay ƒë·ªïi
    flask_app.run(host='0.0.0.0', port=5000, threaded=True, debug=False, use_reloader=False)

def gen_frames():
    global latest_frame, frame_lock, is_running
    while is_running:
        with frame_lock:
            if latest_frame is None:
                time.sleep(0.03)
                continue
            quality = 70 if current_mode == 'camera' else 50
            ret, buffer = cv2.imencode('.jpg', latest_frame, [int(cv2.IMWRITE_JPEG_QUALITY), quality])
            if not ret:
                continue
            jpg = buffer.tobytes()
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + jpg + b'\r\n')
        time.sleep(0.03) 

@flask_app.route('/')
def index():
    icon_name = "Emotion + Posture Detector v3.0 Camera.ico" if current_mode == 'camera' else "Emotion + Posture Detector v3.0 Fullscreen Capture.ico."
    title_text = "Camera" if current_mode == 'camera' else "Fullscreen Capture"
    html_page = HTML_PAGE.replace('{{ title_type }}', title_text).replace('{{ icon_name }}', icon_name)
    return render_template_string(html_page)

@flask_app.route('/video_feed')
def video_feed():
    return Response(gen_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')

# Trang web (Chung)
HTML_PAGE = """
<html>
  <head>
    <link rel="icon" href="{{ url_for('static', filename='{{ icon_name }}') }}" type="image/x-icon">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Emotion + Posture Detector Stream</title>
    <style>
      /* ƒê·∫£m b·∫£o body chi·∫øm to√†n b·ªô viewport */
      html, body {
        height: 100%; /* Chi·ªÅu cao 100% c·ªßa viewport */
        width: 100%;  /* Chi·ªÅu r·ªông 100% c·ªßa viewport */
        margin: 0;
        padding: 0;
        overflow: hidden; /* NgƒÉn cu·ªôn trang n·∫øu ·∫£nh qu√° l·ªõn */
      }

      body {
        background: #a19fa2;
        color: #fff;
        font-family: Arial, sans-serif;
        display: flex;
        flex-direction: column;
        justify-content: center; /* CƒÉn gi·ªØa theo chi·ªÅu d·ªçc */
        align-items: center;    /* CƒÉn gi·ªØa theo chi·ªÅu ngang */
      }

      h2 {
        margin-top: 20px; /* Th√™m kho·∫£ng c√°ch tr√™n cho ti√™u ƒë·ªÅ */
        margin-bottom: 20px;
        flex-shrink: 0; /* ƒê·∫£m b·∫£o ti√™u ƒë·ªÅ kh√¥ng b·ªã co l·∫°i */
      }

      /* Container cho ·∫£nh ƒë·ªÉ n√≥ chi·∫øm kh√¥ng gian c√≤n l·∫°i */
      .video-container {
        flex-grow: 1; /* Cho ph√©p container chi·∫øm h·∫øt kh√¥ng gian c√≤n l·∫°i */
        width: 100%;
        display: flex;
        justify-content: center;
        align-items: center;
        padding: 10px; /* Th√™m padding nh·∫π xung quanh ·∫£nh */
        box-sizing: border-box;
      }

      img {
        /* K√≠ch th∆∞·ªõc t·ªëi ƒëa l√† 100% c·ªßa container ch·ª©a n√≥ */
        max-width: 100%;
        max-height: 100%;
        /* T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc ƒë·ªÉ to√†n b·ªô ·∫£nh hi·ªÉn th·ªã m√† kh√¥ng b·ªã c·∫Øt */
        object-fit: contain;
        /* K√≠ch th∆∞·ªõc ·∫£nh th·ª±c t·∫ø */
        width: auto;
        height: auto;
        /* Gi·ªØ l·∫°i c√°c style g·ªëc */
        border-radius: 10px;
        box-shadow: 0 0 15px rgba(0, 0, 0, 0.3);
      }
    </style>
  </head>
  <body>
    <h2>Emotion + Posture Detector Live - {{ title_type }}</h2>
    <img src="{{ url_for('video_feed') }}">
  </body>
</html>
"""

# H√ÄM H·ªéI C√ì QU√âT TI·∫æP HAY KH√îNG(D√ôNG CHUNG CHO CAMERA V√Ä FULLSCREEN)
def ask_yes_no_blocking(title, message):
    dialog = tk.Toplevel(root)
    dialog.title(title)
    dialog.attributes('-topmost', True)
    dialog.grab_set()  # kh√≥a focus
    dialog.resizable(False, False)

    result = {'value': False}

    def on_yes():
        result['value'] = True
        dialog.destroy()

    def on_no():
        result['value'] = False
        dialog.destroy()

    tk.Label(dialog, text=message, justify='left', wraplength=400)\
        .pack(padx=20, pady=15)

    btn_frame = tk.Frame(dialog)
    btn_frame.pack(pady=10)

    tk.Button(btn_frame, text="Yes", width=10, command=on_yes)\
        .pack(side='left', padx=10)
    tk.Button(btn_frame, text="No", width=10, command=on_no)\
        .pack(side='right', padx=10)

    # CƒÉn gi·ªØa m√†n h√¨nh
    dialog.update_idletasks()
    w = dialog.winfo_width()
    h = dialog.winfo_height()
    x = (dialog.winfo_screenwidth() // 2) - (w // 2)
    y = (dialog.winfo_screenheight() // 2) - (h // 2)
    dialog.geometry(f"+{x}+{y}")

    dialog.wait_window()  # ‚õî BLOCK t·∫°i ƒë√¢y

    return result['value']

# H√ÄM CH√çNH CHO CAMERA (Th√™m logic tho√°t)

def run_detection_camera(cam_index):
    global latest_frame, frame_lock, is_running, root, broadcast_thread, detection_thread
    global cap, DATA_LOGS, SCAN_MIN_DURATION
    global ROI_ACTIVE, ROI_BOX, ROI_DRAWING, ROI_IMAGE_PATH
    global roi_status_color, roi_status_text, roi_emotion_label, ABNORMAL_THRESHOLD
    global class_name, ZONE_ID
    global roi_start, roi_end
    global DISPLAY_SCALE_X, DISPLAY_SCALE_Y
    global force_exit_no_report

    ROI_ACTIVE = False
    ROI_DRAWING = False
    DATA_LOGS = [] # X√≥a log c≈©
    scan_start_time = time.time()
    last_log_time = time.time()

    window_title = 'Emotion + Posture Detector v5.0 (Camera)'
    
    # KHI H√ÄM B·∫ÆT ƒê·∫¶U CH·∫†Y: B√°o cho GUI bi·∫øt l√† detection ƒëang ch·∫°y
    with thread_lock:
        is_running = True

    local_ip = get_local_ip()
    link = f"http://{local_ip}:5000/"
    
    if not hasattr(run_detection_camera, "_flask_started"):
        Thread(target=start_flask_server, daemon=True).start()
        time.sleep(1)
        run_detection_camera._flask_started = True

    if broadcast_thread is None or not broadcast_thread.is_alive():
        broadcast_thread = Thread(target=udp_broadcast, args=(link,), daemon=True)
        broadcast_thread.start()

    update_progress(25, "ƒêang t·∫£i m√¥ h√¨nh nh·∫≠n di·ªán bi·ªÉu c·∫£m (Keras)...")
    import mediapipe as mp
    try:
        from tensorflow.keras.models import load_model
    except ImportError:
        messagebox.showerror("L·ªói", "Vui l√≤ng c√†i ƒë·∫∑t Tensorflow/Keras.")
        root.after(0, destroy_loading_window)
        with thread_lock:
             is_running = False
        return

    face_xml = os.path.join(BASE_DIR, "haarcascade_frontalface_default.xml")
    model_h5 = os.path.join(BASE_DIR, "emotion_detection.h5")
    face_classifier = cv2.CascadeClassifier(face_xml)
    if face_classifier.empty():
        messagebox.showerror("L·ªói", f"Kh√¥ng t√¨m th·∫•y file cascade: {face_xml}")
        root.after(0, destroy_loading_window)
        with thread_lock:
             is_running = False
        return

    classifier = load_model(model_h5)
    update_progress(50, "ƒêang t·∫£i m√¥ h√¨nh t∆∞ th·∫ø (MediaPipe)...")
    class_labels = ['Gi·∫≠n d·ªØ', 'Gh√™ s·ª£', 'S·ª£ h√£i', 'Vui v·∫ª', 'Bu·ªìn', 'B·∫•t ng·ªù', 'Trung l·∫≠p']
    mp_pose = mp.solutions.pose
    pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)
    mp_drawing = mp.solutions.drawing_utils

    update_progress(70, "ƒêang m·ªü camera...")
    cap = cv2.VideoCapture(cam_index, cv2.CAP_DSHOW)
    WIDTH, HEIGHT = 1280, 720
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HEIGHT)
    if not cap.isOpened():
        messagebox.showerror("L·ªói", "Kh√¥ng th·ªÉ m·ªü camera.")
        root.after(0, destroy_loading_window)
        with thread_lock:
             is_running = False
        return

    update_progress(100, "Ho√†n t·∫•t! M·ªü camera...")
    root.after(0, destroy_loading_window)
    root.after(100, show_stream_link, link)
    
    global session_start_time, bad_posture_total_frames, total_detection_frames, history
    session_start_time = time.time()
    bad_posture_total_frames = 0
    total_detection_frames = 0
    history.clear() # ƒê·∫£m b·∫£o history s·∫°ch khi b·∫Øt ƒë·∫ßu phi√™n m·ªõi

    start_time = time.time()
    interval = 120
    scale_factor = 1.0
    first_show = True

    font = ImageFont.truetype(font_path, 28)
    font2 = ImageFont.truetype(font_path, 20)

    short_term_emotion_buffer = deque(maxlen=STABILITY_WINDOW_FRAMES)
    bad_posture_counter = 0 
    current_stable_emotion = 'Trung l·∫≠p'

    status_posture = "Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø"

    force_exit_no_report = False
    # V√≤ng l·∫∑p ch√≠nh
    while cap.isOpened() and is_running:
        ret, frame = cap.read()
        if not ret: break

        current_time = time.time()

        # ... (Ph·∫ßn logic Emotion, Posture Detection v√† Drawing gi·ªØ nguy√™n) ...
        # --- Emotion Detection ---
        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(frame_pil)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_classifier.detectMultiScale(gray, 1.3, 5)

        final_emotion_label = current_stable_emotion
        roi_emotion_label = current_stable_emotion

        for (x, y, w, h) in faces:
            face_cx = x + w // 2
            face_cy = y + h // 2

            face_in_roi = False
            if ROI_ACTIVE and ROI_BOX:
                rx1, ry1, rx2, ry2 = ROI_BOX
                if rx1 <= face_cx <= rx2 and ry1 <= face_cy <= ry2:
                    face_in_roi = True

            cv2.rectangle(frame, (x, y), (x + w, y + h), (242, 248, 68), 2)
            roi_gray = cv2.resize(gray[y:y + h, x:x + w], (48, 48), interpolation=cv2.INTER_AREA)
            roi = np.expand_dims(np.expand_dims(roi_gray.astype("float") / 255.0, axis=-1), axis=0)
            
            # L·∫•y x√°c su·∫•t
            predictions = classifier.predict(roi, verbose=0)
            probabilities = predictions[0] 
            sorted_indices = np.argsort(probabilities)[::-1]
            
            p_max = probabilities[sorted_indices[0]]
            p_top2 = probabilities[sorted_indices[1]]
            predicted_label = class_labels[sorted_indices[0]] # Nh√£n d·ª± ƒëo√°n

            # 1. KI·ªÇM TRA NG∆Ø·ª†NG (VALIDATION) theo DOCX
            config = EMOTION_THRESHOLDS.get(predicted_label, {'p_max': THRESHOLD_P_MAX_DEFAULT, 'delta': THRESHOLD_DELTA_TOP2_DEFAULT})
            
            is_reliable = p_max >= config['p_max']
            is_not_ambiguous = (p_max - p_top2) >= config['delta']
            
            if is_reliable and is_not_ambiguous:
                raw_validated_label = predicted_label
            else:
                raw_validated_label = 'Unknown' # Nh√£n kh√¥ng ƒë·ªß tin c·∫≠y (theo DOCX)

            # S·ª¨A CH·ªÆA ƒê·ªÇ ƒê·ªíNG B·ªò H√ìA LOGIC ·ªîN ƒê·ªäNH C·∫¢M X√öC:
            # 2. C·ª¨A S·ªî ·ªîN ƒê·ªäNH NG·∫ÆN H·∫†N (W: 1 FRAME HO·∫∂C ·ªîN ƒê·ªäNH THEO C·∫§U H√åNH)
            short_term_emotion_buffer.append(raw_validated_label)
            final_emotion_label = current_stable_emotion # Gi·ªØ nh√£n c≈© tr∆∞·ªõc khi c√≥ k·∫øt qu·∫£ m·ªõi

            if len(short_term_emotion_buffer) == STABILITY_WINDOW_FRAMES:
                counts = Counter(short_term_emotion_buffer)
                most_common_label, count = counts.most_common(1)[0]
                
                # N·∫øu nh√£n ph·ªï bi·∫øn nh·∫•t chi·∫øm ∆∞u th·∫ø (>= 80%) v√† kh√¥ng ph·∫£i Unknown
                if most_common_label != 'Unknown' and (count / STABILITY_WINDOW_FRAMES) >= STABILITY_DOMINANCE_RATIO: 
                    final_emotion_label = most_common_label
                # Ng∆∞·ª£c l·∫°i, gi·ªØ nh√£n ·ªïn ƒë·ªãnh tr∆∞·ªõc ƒë√≥
                else: 
                    final_emotion_label = current_stable_emotion 
                
                # C·∫≠p nh·∫≠t nh√£n ·ªïn ƒë·ªãnh hi·ªán t·∫°i v√† l∆∞u v√†o l·ªãch s·ª≠ d√†i h·∫°n
                current_stable_emotion = final_emotion_label
                
                # C·∫≠p nh·∫≠t l·ªãch s·ª≠ d√†i h·∫°n (Ch·ªâ ƒë·ªÉ t√≠nh Negative Ratio)
                if current_stable_emotion != 'Unknown':
                     # 1 = Ti√™u c·ª±c; 0 = T√≠ch c·ª±c/Trung l·∫≠p
                     if current_stable_emotion in ['Gi·∫≠n d·ªØ', 'Gh√™ s·ª£', 'S·ª£ h√£i', 'Bu·ªìn']:
                         history.append(1)
                     else:
                         history.append(0)
                
                # X√≥a buffer ƒë·ªÉ b·∫Øt ƒë·∫ßu c·ª≠a s·ªï m·ªõi
                short_term_emotion_buffer.clear()
            else:
                final_emotion_label = current_stable_emotion # Hi·ªÉn th·ªã nh√£n ·ªïn ƒë·ªãnh tr∆∞·ªõc ƒë√≥
            
            if face_in_roi:
                roi_emotion_label = final_emotion_label
            # V·∫Ω nh√£n c·∫£m x√∫c ƒë√£ ƒë∆∞·ª£c ·ªïn ƒë·ªãnh l√™n frame
            frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(frame_pil)
            draw_text_with_outline(draw, (x, y - 35), final_emotion_label, font, (0, 255, 0))
            frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)

        negative_ratio = sum(history) / len(history) if len(history) > 0 else 0
        elapsed = time.time() - start_time
        #if elapsed >= interval:
        #    if negative_ratio > 0.6:
        #        show_warning("Pause / ƒë·ªïi ho·∫°t ƒë·ªông / ngh·ªâ 2 ph√∫t")
        #    start_time = time.time()
        

        labels = [final_emotion_label]
        # --- Posture Detection ---

        if ROI_ACTIVE and ROI_BOX:
            x1, y1, x2, y2 = ROI_BOX

            # C·∫Øt ·∫£nh theo ROI
            roi_frame = frame[y1:y2, x1:x2]

            if roi_frame.size == 0:
                results = None
            else:
                image_rgb = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2RGB)
                results = pose.process(image_rgb)
        else:
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(image_rgb)   


        angle_back = 0
        status_back_detail = "Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø"
        status_posture = "Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø"
        color = (255, 255, 255)
        angle_back, angle_neck, angle_elbow = 0, 0, 0
        status_back, status_neck, status_elbow = "Kh√¥ng ph√°t hi·ªán", "Kh√¥ng ph√°t hi·ªán", "Kh√¥ng ph√°t hi·ªán"
        status_posture = "Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø"

        if results is not None and results.pose_landmarks:
            landmarks = results.pose_landmarks.landmark
            person_in_roi = False

            if ROI_ACTIVE and ROI_BOX:
                x1, y1, x2, y2 = ROI_BOX
                roi_w = x2 - x1
                roi_h = y2 - y1

                # Landmark ƒëang l√† t·ªça ƒë·ªô TRONG ROI
                cx = int(
                    ((landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x +
                      landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x) * 0.5) * roi_w
                ) + x1

                cy = int(
                    ((landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y +
                      landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y) * 0.5) * roi_h
                ) + y1

                if x1 <= cx <= x2 and y1 <= cy <= y2:
                    person_in_roi = True


            if (not ROI_ACTIVE) or person_in_roi:
                try:
                    ear_l = [
                        landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].x,
                        landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].y
                    ]
                    shoulder_l = [
                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,
                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y
                    ]
                    hip_l = [
                        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,
                        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y
                    ]

                    ear_r = [
                        landmarks[mp_pose.PoseLandmark.RIGHT_EAR.value].x,
                        landmarks[mp_pose.PoseLandmark.RIGHT_EAR.value].y
                    ]
                    shoulder_r = [
                        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,
                        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y
                    ]
                    hip_r = [
                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,
                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y
                    ]

                    vis_l = landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].visibility
                    vis_r = landmarks[mp_pose.PoseLandmark.RIGHT_EAR.value].visibility

                    angle_back = None

                    if vis_l >= vis_r and vis_l > 0.5:
                        angle_back = calculate_angle(hip_l, shoulder_l, ear_l)
                    elif vis_r > 0.5:
                        angle_back = calculate_angle(hip_r, shoulder_r, ear_r)
                    else:
                        angle_back = None  # Kh√¥ng ƒë·ªß tin c·∫≠y


                    # --- Quy ƒë·ªïi Ergonomics (D·ª±a tr√™n G√≥c L∆∞ng) ---
                    if angle_back is None:
                        status_posture = "Kh√¥ng ƒë·ªß d·ªØ li·ªáu"
                        color = (255, 255, 255)

                    else:
                        if angle_back >= 170:
                            status_back_detail = "Ng·ªìi th·∫≥ng (Good)"
                            status_posture = "Ng·ªìi th·∫≥ng (Good)"
                            color = (0, 255, 0) # Xanh
                        elif 150 <= angle_back < 170:
                            status_back_detail = "H∆°i c√∫i (Warning)"
                            status_posture = "H∆°i c√∫i (Warning)"
                            color = (255, 255, 0) # V√†ng
                        else:
                            status_back_detail = "C√∫i nhi·ªÅu (Bad)"
                            status_posture = "C√∫i nhi·ªÅu (Bad)"
                            color = (255, 0, 0) # ƒê·ªè
                    
                    # V·∫Ω Landmarks
                    if ROI_ACTIVE and ROI_BOX:
                        mp_drawing.draw_landmarks(
                            roi_frame,
                            results.pose_landmarks,
                            mp_pose.POSE_CONNECTIONS
                        )
                        # d√°n ROI ƒë√£ v·∫Ω landmark l·∫°i frame g·ªëc
                        frame[y1:y2, x1:x2] = roi_frame
                    else:
                        mp_drawing.draw_landmarks(
                            frame,
                            results.pose_landmarks,
                            mp_pose.POSE_CONNECTIONS
                        )

                except Exception:
                    #X·ª≠ l√Ω khi kh√¥ng t√¨m th·∫•y ƒë·ªß 3 ƒëi·ªÉm (X·∫£y ra khi quay nghi√™ng qu√° nhi·ªÅu)
                    status_back_detail = "Kh√¥ng ƒë·ªß ƒëi·ªÉm (Qu√° nghi√™ng)"
                    status_posture = "Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø"
                    color = (255, 255, 255) # Tr·∫Øng

            if ROI_ACTIVE and person_in_roi:
                # Cho file word
                ROI_LOGS.append({ 
                        "time": time.time(), 
                        "emotion": roi_emotion_label, 
                        "posture": status_posture 
                    })

                # Cho trang web
                current_state = f"{roi_emotion_label}/{status_posture}"
                now = time.time()

                # N·∫øu ch∆∞a c√≥ tr·∫°ng th√°i ‚Üí kh·ªüi t·∫°o
                if ROI_STATE_TRACKER["state"] is None:
                    ROI_STATE_TRACKER["state"] = current_state
                    ROI_STATE_TRACKER["start_time"] = now

                # N·∫øu tr·∫°ng th√°i THAY ƒê·ªîI
                elif current_state != ROI_STATE_TRACKER["state"]:
                    prev_state = ROI_STATE_TRACKER["state"]
                    start_time_roi = ROI_STATE_TRACKER["start_time"]
                    duration = int(now - start_time_roi)

                    # Ki·ªÉm tra tr·∫°ng th√°i c≈© c√≥ b·∫•t th∆∞·ªùng kh√¥ng
                    is_abnormal = (
                        "Warning" in prev_state or "Bad" in prev_state or
                        any(x in prev_state for x in ["Bu·ªìn", "Gi·∫≠n d·ªØ", "Gh√™ s·ª£", "S·ª£ h√£i"])
                    )

                    # N·∫øu b·∫•t th∆∞·ªùng v√† ƒë·ªß th·ªùi gian ‚Üí G·ª¨I NGAY
                    if is_abnormal and duration >= ABNORMAL_THRESHOLD:
                        now_dt = datetime.datetime.now()
                        start_time_str = (now_dt - datetime.timedelta(seconds=duration)).strftime('%H:%M:%S')

                        send_incident(
                            prev_state,
                            start_time_str,
                            duration
                        )

                    # Reset sang tr·∫°ng th√°i m·ªõi
                    ROI_STATE_TRACKER["state"] = current_state
                    ROI_STATE_TRACKER["start_time"] = now


        if "Bad" in status_posture:
            bad_posture_counter += 1
        else:
            bad_posture_counter = 0

        if bad_posture_counter >= BAD_POSTURE_WARNING_FRAMES:
            show_warning("C·∫¢NH B√ÅO T∆Ø TH·∫æ: B·∫°n ƒë√£ ng·ªìi c√∫i g√π qu√° l√¢u. Vui l√≤ng ƒëi·ªÅu ch·ªânh l·∫°i t∆∞ th·∫ø ng·ªìi!")
            bad_posture_counter = 0

        total_detection_frames += 1 # TƒÉng t·ªïng frame

        box_color = (0, 0, 255) if negative_ratio > 0.6 else (0, 255, 255) if 0.2 <= negative_ratio <= 0.6 else (0, 255, 0)

        # --- LOGGING D·ªÆ LI·ªÜU ---
        # Ghi log m·ªói 0.5 gi√¢y ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n
        if current_time - last_log_time >= 0.5 and labels:
            with LOG_LOCK:
                # S·ª¨ D·ª§NG status_posture ƒê√É ƒê∆Ø·ª¢C X√ÅC ƒê·ªäNH
                DATA_LOGS.append({
                    'timestamp': current_time,
                    'emotion': labels[0] if labels else 'Kh√¥ng ph√°t hi·ªán', 
                    'posture_status': status_posture,
                })
            last_log_time = current_time

        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(frame_pil)

        draw_text_with_outline(draw, (20, 20), f"T∆∞ th·∫ø L∆∞ng: {status_back_detail} ({int(angle_back)}¬∞)", font2, (255, 255, 255))
        draw_text_with_outline(draw, (20, 50), f"Tr·∫°ng th√°i t·ªïng: {status_posture}", font2, color) # Hi·ªÉn th·ªã t√≥m t·∫Øt v√† m√†u
        draw_text_with_outline(draw, (20, 80), f"S·ªë l∆∞·ª£ng: {len(faces)}", font2, (255, 0, 255))
        draw_text_with_outline(draw, (20, 110), "Tr·∫°ng th√°i:", font2, (0, 0, 255))
        draw_text_with_outline(draw, (960, 680), "B·∫•m ph√≠m 'Q' ƒë·ªÉ tho√°t", font, (255, 255, 0)) # Ch√∫ th√≠ch thay ƒë·ªïi
        draw_text_with_outline(draw, (1000, 20), "B·∫•m ph√≠m 'M' ƒë·ªÉ ph√≥ng to", font2, (255, 255, 0))
        draw_text_with_outline(draw, (1000, 50), "B·∫•m ph√≠m 'N' ƒë·ªÉ thu nh·ªè", font2, (255, 255, 0))
        draw_text_with_outline(draw, (1000, 80), "ƒêang qu√©t t·∫°i: " + class_name, font2, (0, 255, 0))
        draw_text_with_outline(draw, (1000, 110), roi_status_text, font2, roi_status_color)
        frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)
        draw_filled_rectangle_with_outline(frame, (195 - 65, 125 - 13), (215 - 65, 145 - 13), box_color, outline_width=2)
        
        # ... (Ph·∫ßn x·ª≠ l√Ω ph√≠m b·∫•m v√† hi·ªÉn th·ªã gi·ªØ nguy√™n) ...
        key = cv2.waitKey(10) & 0xFF
        if key == ord('m'): scale_factor = min(1.0, scale_factor + 0.1)
        elif key == ord('n'): scale_factor = max(0.2, scale_factor - 0.1)
        elif key == ord('q'):
            current_time = time.time()
            elapsed = current_time - scan_start_time

            # CH∆ØA ƒë·ªß th·ªùi gian qu√©t
            if elapsed < SCAN_MIN_DURATION:
                remaining = int(SCAN_MIN_DURATION - elapsed)

                answer = ask_yes_no_blocking(
                    "Ch∆∞a ƒë·ªß th·ªùi gian qu√©t",
                    f"C·∫ßn qu√©t t·ªëi thi·ªÉu {SCAN_MIN_DURATION} gi√¢y.\n"
                    f"B·∫°n c·∫ßn qu√©t th√™m {remaining} gi√¢y n·ªØa.\n\n"
                    f"N·∫øu d·ª´ng b√¢y gi·ªù s·∫Ω KH√îNG xu·∫•t b√°o c√°o.\n"
                    f"B·∫°n c√≥ ch·∫Øc ch·∫Øn mu·ªën d·ª´ng kh√¥ng?"
                )

                if answer:
                    force_exit_no_report = True
                    break   # tho√°t v√≤ng l·∫∑p CV2
                else:
                    continue  # ti·∫øp t·ª•c qu√©t

            # ƒê√É ƒë·ªß th·ªùi gian qu√©t
            else:
                break

        elif key == ord('v') and not ROI_ACTIVE:
            ROI_DRAWING = not ROI_DRAWING

            # CH·ªà xo√° khung khi T·∫ÆT v·∫Ω
            if not ROI_DRAWING:
                roi_start = None
                roi_end = None
                ROI_BOX = None

        elif key == ord('s') and ROI_BOX and not ROI_ACTIVE:
            global ROI_IMAGE_BUFFER
            if not ZONE_ID:
                ok = ask_student_id(root)

                # √âP c·ª≠a s·ªï OpenCV hi·ªán l·∫°i sau khi ƒë√≥ng Tkinter
                cv2.namedWindow(window_title)
                hwnd = win32gui.FindWindow(None, window_title)
                if hwnd:
                    win32gui.SetWindowPos(
                        hwnd,
                        win32con.HWND_TOPMOST,
                        0, 0, 0, 0,
                        win32con.SWP_NOMOVE | win32con.SWP_NOSIZE
                    )

                if not ok:
                    show_warning("Vui l√≤ng nh·∫≠p Student ID tr∆∞·ªõc khi qu√©t ROI.")
                    continue


            ROI_ACTIVE = True
            ROI_LOGS.clear()
            roi_scan_start_time = time.time()

            x1, y1, x2, y2 = ROI_BOX
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)

            roi_crop = frame[y1:y2, x1:x2]

            if roi_crop.size > 0:
                _, buffer = cv2.imencode(".png", roi_crop)
                ROI_IMAGE_BUFFER = buffer.tobytes()

            show_warning("B·∫ÆT ƒê·∫¶U QU√âT ROI (ƒê√É CH·ª§P ·∫¢NH ROI)")


        elif key == ord('e') and ROI_ACTIVE:
            # G·ª¨I TR·∫†NG TH√ÅI CU·ªêI C√ôNG N·∫æU ƒê·ª¶ ƒêI·ªÄU KI·ªÜN
            if ROI_STATE_TRACKER["state"] and ROI_STATE_TRACKER["start_time"]:
                now = time.time()
                duration = now - ROI_STATE_TRACKER["start_time"]

                if duration >= ABNORMAL_THRESHOLD:
                    duration = int(time.time() - ROI_STATE_TRACKER["start_time"])

                    now_dt = datetime.datetime.now()
                    start_time_str = (now_dt - datetime.timedelta(seconds=duration)).strftime('%H:%M:%S')

                    send_incident(
                        ROI_STATE_TRACKER["state"],
                        start_time_str,
                        duration
                    )

            ROI_ACTIVE = False
            ROI_STATE_TRACKER["state"] = None
            ROI_STATE_TRACKER["start_time"] = None

            export_roi_to_word()
            ROI_IMAGE_PATH = None
            ZONE_ID = None

        if ROI_DRAWING:
            roi_status_text = "V·∫Ω khung ROI: B·∫¨T"
            roi_status_color = (0, 255, 0)
        else:
            roi_status_text = "V·∫Ω khung ROI: T·∫ÆT"
            roi_status_color = (255, 0, 0)


        frame_stream = cv2.resize(frame.copy(), (int(frame.shape[1]*0.6), int(frame.shape[0]*0.6)))
        with frame_lock:
            latest_frame = frame_stream.copy()

        new_w, new_h = int(WIDTH * scale_factor), int(HEIGHT * scale_factor)

        DISPLAY_SCALE_X = WIDTH / new_w
        DISPLAY_SCALE_Y = HEIGHT / new_h

        if ROI_DRAWING and not ROI_ACTIVE and roi_start and roi_end:
            cv2.rectangle(frame, roi_start, roi_end, (255,255,0), 2) # Xanh da tr·ªùi


        # ƒê√£ th·∫£ chu·ªôt ‚Üí khung V√ÄNG + ch·ªØ
        if ROI_BOX and ROI_DRAWING:
            x1, y1, x2, y2 = ROI_BOX
            cv2.rectangle(
                frame,
                (x1, y1),
                (x2, y2),
                (0, 255, 255) if not ROI_ACTIVE else (0, 255, 0),  # V√†ng
                2
            )

            cv2.putText(
                frame,
                f"ROI ACTIVE | HS-{ZONE_ID}" if ROI_ACTIVE else "ROI DRAWN",
                (x1, y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (0, 255, 255) if not ROI_ACTIVE else (0, 255, 0),
                2
            )

        cv2.imshow(window_title, cv2.resize(frame, (new_w, new_h)))


        camera_icon = os.path.join(BASE_DIR, "Emotion + Posture Detector v3.0 Camera.ico")

        if first_show:
            bring_window_to_front(window_title)
            set_opencv_window_icon(window_title, camera_icon)
            first_show = False

        cv2.namedWindow(window_title)
        cv2.setMouseCallback(window_title, mouse_draw_roi)

        if first_show:
            bring_window_to_front(window_title)
            first_show = False
        else:
            # V·∫´n gi·ªØ c·ª≠a s·ªï CV2 lu√¥n tr√™n c√πng (n·∫øu c√≥)
            hwnd = win32gui.FindWindow(None, window_title)
            if hwnd: win32gui.SetWindowPos(hwnd, win32con.HWND_TOPMOST, 0, 0, 0, 0, win32con.SWP_NOMOVE | win32con.SWP_NOSIZE)
        
    # KHI V√íNG L·∫∂P K·∫æT TH√öC: D·ªçn d·∫πp
    cap.release()
    cv2.destroyAllWindows()
    with thread_lock:
        is_running = False
        detection_thread = None # ƒê·∫∑t l·∫°i lu·ªìng ƒë·ªÉ c√≥ th·ªÉ ch·∫°y l·∫°i

    if len(DATA_LOGS) > 1 and not force_exit_no_report:
        root.after(100, analyze_and_export_csv) # Ch·∫°y h√†m xu·∫•t CSV tr√™n lu·ªìng ch√≠nh Tkinter

# H√ÄM CH√çNH CHO FULLSCREEN (Th√™m logic tho√°t)

def run_detection_fullscreen():
    global latest_frame, frame_lock, is_running, root, broadcast_thread, detection_thread
    global cap, DATA_LOGS, SCAN_MIN_DURATION, WIDTH_SCR, HEIGHT_SCR # TH√äM WIDTH_SCR, HEIGHT_SCR
    global INCIDENT_STATE, INCIDENT_START_TIME, INCIDENT_START_TIME_STR
    global ROI_ACTIVE, ROI_BOX, ROI_DRAWING, ROI_IMAGE_PATH
    global roi_status_color, roi_status_text, roi_emotion_label, ABNORMAL_THRESHOLD
    global roi_start, roi_end, scale_factor
    global class_name, ZONE_ID
    global force_exit_no_report

    ROI_ACTIVE = False
    ROI_DRAWING = False
    roi_start = None
    roi_end = None

    INCIDENT_STATE = None
    INCIDENT_START_TIME = None
    INCIDENT_START_TIME_STR = None

    DATA_LOGS = [] # X√≥a log c≈©
    scan_start_time = time.time()
    last_log_time = time.time()

    window_title = 'Emotion + Posture Detector v5.0 (Fullscreen Capture)'
    
    # KHI H√ÄM B·∫ÆT ƒê·∫¶U CH·∫†Y: B√°o cho GUI bi·∫øt l√† detection ƒëang ch·∫°y
    with thread_lock:
        is_running = True
    
    local_ip = get_local_ip()
    link = f"http://{local_ip}:5000/"
    
    if not hasattr(run_detection_fullscreen, "_flask_started"):
        Thread(target=start_flask_server, daemon=True).start()
        time.sleep(1)
        run_detection_fullscreen._flask_started = True

    if broadcast_thread is None or not broadcast_thread.is_alive():
        broadcast_thread = Thread(target=udp_broadcast, args=(link,), daemon=True)
        broadcast_thread.start()

    # show_loading_window("ƒêang kh·ªüi ƒë·ªông Fullscreen Capture...")
    update_progress(25, "ƒêang t·∫£i m√¥ h√¨nh nh·∫≠n di·ªán bi·ªÉu c·∫£m (Keras)...")
    import mediapipe as mp
    try:
        from tensorflow.keras.models import load_model
    except ImportError:
        messagebox.showerror("L·ªói", "Vui l√≤ng c√†i ƒë·∫∑t Tensorflow/Keras.")
        root.after(0, destroy_loading_window)
        with thread_lock:
             is_running = False
        return

    face_xml = os.path.join(BASE_DIR, "haarcascade_frontalface_default.xml")
    model_h5 = os.path.join(BASE_DIR, "emotion_detection.h5")
    face_classifier = cv2.CascadeClassifier(face_xml)
    if face_classifier.empty():
        messagebox.showerror("L·ªói", f"Kh√¥ng t√¨m th·∫•y file cascade: {face_xml}")
        root.after(0, destroy_loading_window)
        with thread_lock:
             is_running = False
        return
    classifier = load_model(model_h5)
    
    update_progress(50, "ƒêang t·∫£i m√¥ h√¨nh t∆∞ th·∫ø (MediaPipe)...")
    class_labels = ['Gi·∫≠n d·ªØ', 'Gh√™ s·ª£', 'S·ª£ h√£i', 'Vui v·∫ª', 'Bu·ªìn', 'B·∫•t ng·ªù', 'Trung l·∫≠p']
    mp_pose = mp.solutions.pose
    pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)
    mp_drawing = mp.solutions.drawing_utils

    update_progress(100, "Ho√†n t·∫•t! M·ªü m√†n h√¨nh...")
    root.after(0, destroy_loading_window)
    root.after(100, show_stream_link, link)

    WIDTH_SCR, HEIGHT_SCR = pyautogui.size() # L·∫•y k√≠ch th∆∞·ªõc m√†n h√¨nh
    scale_factor = 0.5
    first_show = True

    global session_start_time, bad_posture_total_frames, total_detection_frames, history
    session_start_time = time.time()
    bad_posture_total_frames = 0
    total_detection_frames = 0
    history.clear() # ƒê·∫£m b·∫£o history s·∫°ch khi b·∫Øt ƒë·∫ßu phi√™n m·ªõi

    start_time = time.time()
    interval = 120

    font = ImageFont.truetype(font_path, 28)
    font2 = ImageFont.truetype(font_path, 20)

    short_term_emotion_buffer = deque(maxlen=STABILITY_WINDOW_FRAMES)
    bad_posture_counter = 0 
    current_stable_emotion = 'Trung l·∫≠p'

    status_posture = "Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø"
        
    force_exit_no_report = False
    # V√≤ng l·∫∑p ch√≠nh
    while is_running:

        current_time = time.time()
        frame = np.array(pyautogui.screenshot())
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

        # === B·∫ÆT ƒê·∫¶U: Logic che c·ª≠a s·ªï OpenCV (ƒê·ªìng b·ªô v·ªõi logic c≈© c·ªßa Fullscreen) ===
        hwnd = win32gui.FindWindow(None, window_title)
        if hwnd:
            try:
                rect = win32gui.GetWindowRect(hwnd)
                x_win, y_win, x_end_win, y_end_win = rect
                # ƒê·∫£m b·∫£o t·ªça ƒë·ªô h·ª£p l·ªá
                x_win, y_win = max(0, x_win), max(0, y_win)
                x_end_win, y_end_win = min(WIDTH_SCR, x_end_win), min(HEIGHT_SCR, y_end_win)
                
                # 2. V·∫Ω ƒë√® (l√†m ƒëen) khu v·ª±c c·ª≠a s·ªï OpenCV
                if x_end_win > x_win and y_end_win > y_win:
                    # Ch·ª•p m√†n h√¨nh th∆∞·ªùng c√≥ k√™nh m√†u BGR, kh√¥ng ph·∫£i RGB
                    frame[y_win:y_end_win, x_win:x_end_win] = (0, 0, 0) # M√†u ƒëen BGR
            except Exception:
                # B·ªè qua n·∫øu c√≥ l·ªói khi l·∫•y t·ªça ƒë·ªô c·ª≠a s·ªï
                pass
        # === K·∫æT TH√öC: Logic che c·ª≠a s·ªï OpenCV ===
        
        # --- Emotion Detection (ƒê√£ ƒë·ªìng b·ªô) ---
        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(frame_pil)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_classifier.detectMultiScale(gray, 1.3, 5)
        
        final_emotion_label = current_stable_emotion
        roi_emotion_label = current_stable_emotion

        for (x, y, w, h) in faces:
            face_cx = x + w // 2
            face_cy = y + h // 2

            face_in_roi = False
            if ROI_ACTIVE and ROI_BOX:
                rx1, ry1, rx2, ry2 = ROI_BOX
                if rx1 <= face_cx <= rx2 and ry1 <= face_cy <= ry2:
                    face_in_roi = True
            cv2.rectangle(frame, (x, y), (x + w, y + h), (242, 248, 68), 2)
            roi_gray = cv2.resize(gray[y:y + h, x:x + w], (48, 48), interpolation=cv2.INTER_AREA)
            roi = np.expand_dims(np.expand_dims(roi_gray.astype("float") / 255.0, axis=-1), axis=0)
            
            # L·∫•y x√°c su·∫•t
            predictions = classifier.predict(roi, verbose=0)
            probabilities = predictions[0] 
            sorted_indices = np.argsort(probabilities)[::-1]
            
            p_max = probabilities[sorted_indices[0]]
            p_top2 = probabilities[sorted_indices[1]]
            predicted_label = class_labels[sorted_indices[0]] # Nh√£n d·ª± ƒëo√°n

            # 1. KI·ªÇM TRA NG∆Ø·ª†NG (VALIDATION) theo DOCX
            config = EMOTION_THRESHOLDS.get(predicted_label, {'p_max': THRESHOLD_P_MAX_DEFAULT, 'delta': THRESHOLD_DELTA_TOP2_DEFAULT})
            
            is_reliable = p_max >= config['p_max']
            is_not_ambiguous = (p_max - p_top2) >= config['delta']
            
            if is_reliable and is_not_ambiguous:
                raw_validated_label = predicted_label
            else:
                raw_validated_label = 'Unknown' # Nh√£n kh√¥ng ƒë·ªß tin c·∫≠y (theo DOCX)

            # 2. C·ª¨A S·ªî ·ªîN ƒê·ªäNH NG·∫ÆN H·∫†N (W: 2-3 GI√ÇY)
            short_term_emotion_buffer.append(raw_validated_label)
            
            if len(short_term_emotion_buffer) == STABILITY_WINDOW_FRAMES:
                counts = Counter(short_term_emotion_buffer)
                most_common_label, count = counts.most_common(1)[0]
                
                # N·∫øu nh√£n ph·ªï bi·∫øn nh·∫•t chi·∫øm ∆∞u th·∫ø (>= 80%) v√† kh√¥ng ph·∫£i Unknown
                if most_common_label != 'Unknown' and (count / STABILITY_WINDOW_FRAMES) >= STABILITY_DOMINANCE_RATIO: 
                    final_emotion_label = most_common_label
                # Ng∆∞·ª£c l·∫°i, gi·ªØ nh√£n ·ªïn ƒë·ªãnh tr∆∞·ªõc ƒë√≥
                else: 
                    final_emotion_label = current_stable_emotion 
                
                # C·∫≠p nh·∫≠t nh√£n ·ªïn ƒë·ªãnh hi·ªán t·∫°i v√† l∆∞u v√†o l·ªãch s·ª≠ d√†i h·∫°n
                current_stable_emotion = final_emotion_label
                if current_stable_emotion != 'Unknown':
                     # 1 = Ti√™u c·ª±c; 0 = T√≠ch c·ª±c/Trung l·∫≠p
                     if current_stable_emotion in ['Gi·∫≠n d·ªØ', 'Gh√™ s·ª£', 'S·ª£ h√£i', 'Bu·ªìn']:
                         history.append(1)
                     else:
                         history.append(0)
                
                # X√≥a buffer ƒë·ªÉ b·∫Øt ƒë·∫ßu c·ª≠a s·ªï m·ªõi
                short_term_emotion_buffer.clear()
            else:
                final_emotion_label = current_stable_emotion # Hi·ªÉn th·ªã nh√£n ·ªïn ƒë·ªãnh tr∆∞·ªõc ƒë√≥
            
            if face_in_roi:
                roi_emotion_label = final_emotion_label
            # V·∫Ω nh√£n c·∫£m x√∫c ƒë√£ ƒë∆∞·ª£c ·ªïn ƒë·ªãnh l√™n frame
            frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(frame_pil)
            draw_text_with_outline(draw, (x, y - 35), final_emotion_label, font, (0, 255, 0))
            frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)
        
        negative_ratio = sum(history) / len(history) if len(history) > 0 else 0
        elapsed = time.time() - start_time
        if elapsed >= interval:
            if negative_ratio > 0.6:
                show_warning("Pause / ƒë·ªïi ho·∫°t ƒë·ªông / ngh·ªâ 2 ph√∫t")
            start_time = time.time()

        labels = [final_emotion_label]
        # --- Posture Detection (ƒê√£ ƒë·ªìng b·ªô) ---
        if ROI_ACTIVE and ROI_BOX:
            x1, y1, x2, y2 = ROI_BOX

            # C·∫Øt ·∫£nh theo ROI
            roi_frame = frame[y1:y2, x1:x2]

            if roi_frame.size == 0:
                results = None
            else:
                image_rgb = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2RGB)
                results = pose.process(image_rgb)
        else:
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(image_rgb)
        angle_back = 0
        status_back_detail = "Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø"
        status_posture = "Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø"
        color = (255, 255, 255)
        
        if results is not None and results.pose_landmarks:
            landmarks = results.pose_landmarks.landmark
            
            person_in_roi = False

            if ROI_ACTIVE and ROI_BOX:
                x1, y1, x2, y2 = ROI_BOX
                roi_w = x2 - x1
                roi_h = y2 - y1

                # Landmark ƒëang l√† t·ªça ƒë·ªô TRONG ROI
                cx = int(
                    ((landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x +
                      landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x) * 0.5) * roi_w
                ) + x1

                cy = int(
                    ((landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y +
                      landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y) * 0.5) * roi_h
                ) + y1

                if x1 <= cx <= x2 and y1 <= cy <= y2:
                    person_in_roi = True

            if (not ROI_ACTIVE) or person_in_roi:
                try:
                    ear_l = [
                        landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].x,
                        landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].y
                    ]
                    shoulder_l = [
                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,
                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y
                    ]
                    hip_l = [
                        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,
                        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y
                    ]

                    ear_r = [
                        landmarks[mp_pose.PoseLandmark.RIGHT_EAR.value].x,
                        landmarks[mp_pose.PoseLandmark.RIGHT_EAR.value].y
                    ]
                    shoulder_r = [
                        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,
                        landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y
                    ]
                    hip_r = [
                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,
                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y
                    ]

                    vis_l = landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].visibility
                    vis_r = landmarks[mp_pose.PoseLandmark.RIGHT_EAR.value].visibility

                    angle_back = None

                    if vis_l >= vis_r and vis_l > 0.5:
                        angle_back = calculate_angle(hip_l, shoulder_l, ear_l)
                    elif vis_r > 0.5:
                        angle_back = calculate_angle(hip_r, shoulder_r, ear_r)
                    else:
                        angle_back = None  # Kh√¥ng ƒë·ªß tin c·∫≠y


                    # --- Quy ƒë·ªïi Ergonomics (D·ª±a tr√™n G√≥c L∆∞ng) ---
                    if angle_back is None:
                        status_posture = "Kh√¥ng ƒë·ªß d·ªØ li·ªáu"
                        color = (255, 255, 255)

                    else:
                        if angle_back >= 170:
                            status_back_detail = "Ng·ªìi th·∫≥ng (Good)"
                            status_posture = "Ng·ªìi th·∫≥ng (Good)"
                            color = (0, 255, 0) # Xanh
                        elif 150 <= angle_back < 170:
                            status_back_detail = "H∆°i c√∫i (Warning)"
                            status_posture = "H∆°i c√∫i (Warning)"
                            color = (255, 255, 0) # V√†ng
                        else:
                            status_back_detail = "C√∫i nhi·ªÅu (Bad)"
                            status_posture = "C√∫i nhi·ªÅu (Bad)"
                            color = (255, 0, 0) # ƒê·ªè
                    
                    # V·∫Ω Landmarks
                    if ROI_ACTIVE and ROI_BOX:
                        mp_drawing.draw_landmarks(
                            roi_frame,
                            results.pose_landmarks,
                            mp_pose.POSE_CONNECTIONS
                        )
                        # d√°n ROI ƒë√£ v·∫Ω landmark l·∫°i frame g·ªëc
                        frame[y1:y2, x1:x2] = roi_frame
                    else:
                        mp_drawing.draw_landmarks(
                            frame,
                            results.pose_landmarks,
                            mp_pose.POSE_CONNECTIONS
                        )
                except Exception:
                    #X·ª≠ l√Ω khi kh√¥ng t√¨m th·∫•y ƒë·ªß 3 ƒëi·ªÉm (X·∫£y ra khi quay nghi√™ng qu√° nhi·ªÅu)
                    status_back_detail = "Kh√¥ng ƒë·ªß ƒëi·ªÉm (Qu√° nghi√™ng)"
                    status_posture = "Kh√¥ng ph√°t hi·ªán t∆∞ th·∫ø"
                    color = (255, 255, 255) # Tr·∫Øng

            if ROI_ACTIVE and person_in_roi:
                # Cho file word
                ROI_LOGS.append({ 
                        "time": time.time(), 
                        "emotion": roi_emotion_label, 
                        "posture": status_posture 
                    })

                # Cho trang web
                current_state = f"{roi_emotion_label}/{status_posture}"
                now = time.time()

                # N·∫øu ch∆∞a c√≥ tr·∫°ng th√°i ‚Üí kh·ªüi t·∫°o
                if ROI_STATE_TRACKER["state"] is None:
                    ROI_STATE_TRACKER["state"] = current_state
                    ROI_STATE_TRACKER["start_time"] = now

                # N·∫øu tr·∫°ng th√°i THAY ƒê·ªîI
                elif current_state != ROI_STATE_TRACKER["state"]:
                    prev_state = ROI_STATE_TRACKER["state"]
                    start_time_roi = ROI_STATE_TRACKER["start_time"]
                    duration = int(now - start_time_roi)

                    # Ki·ªÉm tra tr·∫°ng th√°i c≈© c√≥ b·∫•t th∆∞·ªùng kh√¥ng
                    is_abnormal = (
                        "Warning" in prev_state or "Bad" in prev_state or
                        any(x in prev_state for x in ["Bu·ªìn", "Gi·∫≠n d·ªØ", "Gh√™ s·ª£", "S·ª£ h√£i"])
                    )

                    # N·∫øu b·∫•t th∆∞·ªùng v√† ƒë·ªß th·ªùi gian ‚Üí G·ª¨I NGAY
                    if is_abnormal and duration >= ABNORMAL_THRESHOLD:
                        now_dt = datetime.datetime.now()
                        start_time_str = (now_dt - datetime.timedelta(seconds=duration)).strftime('%H:%M:%S')

                        send_incident(
                            prev_state,
                            start_time_str,
                            duration
                        )

                    # Reset sang tr·∫°ng th√°i m·ªõi
                    ROI_STATE_TRACKER["state"] = current_state
                    ROI_STATE_TRACKER["start_time"] = now

        now_time = time.time()
        now = datetime.datetime.now()

        combined_state = f"{final_emotion_label}/{status_posture}"

        is_abnormal = (
            final_emotion_label in ['Bu·ªìn', 'Gi·∫≠n d·ªØ', 'S·ª£ h√£i', 'Gh√™ s·ª£']
            or "Bad" in status_posture
            or "Warning" in status_posture
        )

        if ROI_ACTIVE and is_abnormal:

            # === TR·∫†NG TH√ÅI M·ªöI ===
            if INCIDENT_STATE != combined_state:

                # N·∫øu c√≥ tr·∫°ng th√°i c≈© ‚Üí ki·ªÉm tra g·ª≠i
                if INCIDENT_STATE is not None:
                    duration = int(now_time - INCIDENT_START_TIME)
                    if duration >= ABNORMAL_THRESHOLD:
                        send_incident(
                            INCIDENT_STATE,
                            INCIDENT_START_TIME_STR,
                            duration
                        )

                # B·∫Øt ƒë·∫ßu tr·∫°ng th√°i m·ªõi
                INCIDENT_STATE = combined_state
                INCIDENT_START_TIME = now_time
                INCIDENT_START_TIME_STR = now.strftime('%H:%M:%S')

            else:
                # === TR·∫†NG TH√ÅI GI·ªÆ NGUY√äN ===
                duration = int(now_time - INCIDENT_START_TIME)
                if duration >= ABNORMAL_THRESHOLD:
                    send_incident(
                        INCIDENT_STATE,
                        INCIDENT_START_TIME_STR,
                        duration
                    )

                    # reset ƒë·ªÉ tr√°nh spam
                    INCIDENT_START_TIME = now_time
                    INCIDENT_START_TIME_STR = now.strftime('%H:%M:%S')

        else:
            # === H·∫æT B·∫§T TH∆Ø·ªúNG ‚Üí g·ª≠i n·ªët n·∫øu c·∫ßn ===
            if INCIDENT_STATE is not None:
                duration = int(now_time - INCIDENT_START_TIME)
                if duration >= ABNORMAL_THRESHOLD:
                    send_incident(
                        INCIDENT_STATE,
                        INCIDENT_START_TIME_STR,
                        duration
                    )

            INCIDENT_STATE = None
            INCIDENT_START_TIME = None
            INCIDENT_START_TIME_STR = None

        # --- LOGIC C·∫¢NH B√ÅO T∆Ø TH·∫æ ƒê·ªòC L·∫¨P (ƒê√£ ƒë·ªìng b·ªô) ---
        if "Bad" in status_posture:
            bad_posture_counter += 1
        else:
            bad_posture_counter = 0

        if bad_posture_counter >= BAD_POSTURE_WARNING_FRAMES:
            show_warning("C·∫¢NH B√ÅO T∆Ø TH·∫æ: B·∫°n ƒë√£ ng·ªìi c√∫i g√π qu√° l√¢u. Vui l√≤ng ƒëi·ªÅu ch·ªânh l·∫°i t∆∞ th·∫ø ng·ªìi!")
            bad_posture_counter = 0 # Reset sau c·∫£nh b√°o

        total_detection_frames += 1 # TƒÉng t·ªïng frame

        box_color = (0, 0, 255) if negative_ratio > 0.6 else (0, 255, 255) if 0.2 <= negative_ratio <= 0.6 else (0, 255, 0)


        # --- LOGGING D·ªÆ LI·ªÜU ---
        # Ghi log m·ªói 0.5 gi√¢y ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n
        if current_time - last_log_time >= 0.5 and labels:
            with LOG_LOCK:
                # S·ª¨ D·ª§NG status_posture ƒê√É ƒê∆Ø·ª¢C X√ÅC ƒê·ªäNH
                DATA_LOGS.append({
                    'timestamp': current_time,
                    'emotion': labels[0] if labels else 'Kh√¥ng ph√°t hi·ªán', 
                    'posture_status': status_posture,
                })
            last_log_time = current_time

        # --- DRAWING TEXT (ƒê√£ ƒë·ªìng b·ªô) ---
        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(frame_pil)

        # L∆ØU √ù: Thay ƒë·ªïi v·ªã tr√≠ text ƒë·ªÉ ph√π h·ª£p v·ªõi m√†n h√¨nh l·ªõn
        draw_text_with_outline(draw, (20, 20), f"T∆∞ th·∫ø L∆∞ng: {status_back_detail} ({int(angle_back)}¬∞)", font2, (255, 255, 255))
        draw_text_with_outline(draw, (20, 50), f"Tr·∫°ng th√°i t·ªïng: {status_posture}", font2, color) # Hi·ªÉn th·ªã t√≥m t·∫Øt v√† m√†u
        draw_text_with_outline(draw, (20, 80), f"S·ªë l∆∞·ª£ng: {len(faces)}", font2, (255, 0, 255))
        draw_text_with_outline(draw, (20, 110), "Tr·∫°ng th√°i:", font2, (0, 0, 255))
        draw_text_with_outline(draw, (int(WIDTH_SCR * 0.75 + 145), HEIGHT_SCR - 40), "B·∫•m ph√≠m 'Q' ƒë·ªÉ tho√°t", font, (255, 255, 0)) # Ch√∫ th√≠ch thay ƒë·ªïi
        draw_text_with_outline(draw, (int(WIDTH_SCR * 0.8 + 90), 20), "B·∫•m ph√≠m 'M' ƒë·ªÉ ph√≥ng to", font2, (255, 255, 0))
        draw_text_with_outline(draw, (int(WIDTH_SCR * 0.8 + 90), 50), "B·∫•m ph√≠m 'N' ƒë·ªÉ thu nh·ªè", font2, (255, 255, 0))
        draw_text_with_outline(draw, (int(WIDTH_SCR * 0.8 + 90), 80), "ƒêang qu√©t t·∫°i: " + class_name, font2, (0, 255, 0))
        draw_text_with_outline(draw, (int(WIDTH_SCR * 0.8 + 90), 110), roi_status_text, font2, roi_status_color)
        frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)

        # Draw box m√†u (ƒêi·ªÅu ch·ªânh t·ªça ƒë·ªô cho ph√π h·ª£p)
        draw_filled_rectangle_with_outline(frame, (235 - 100, 125 - 13), (255 - 100, 145 - 13), box_color, outline_width=2) 
        
        # ... (Ph·∫ßn x·ª≠ l√Ω ph√≠m b·∫•m v√† hi·ªÉn th·ªã gi·ªØ nguy√™n) ...
        key = cv2.waitKey(10) & 0xFF
        if key == ord('m'): scale_factor = min(0.9, scale_factor + 0.1)
        elif key == ord('n'): scale_factor = max(0.2, scale_factor - 0.1)
        elif key == ord('q'): 
            # B·∫Øt bu·ªôc ph·∫£i qu√©t t·ªëi thi·ªÉu 30s
            current_time = time.time()
            elapsed = current_time - scan_start_time

            if elapsed < SCAN_MIN_DURATION:
                remaining = int(SCAN_MIN_DURATION - elapsed)

                answer = ask_yes_no_blocking(
                    "Ch∆∞a ƒë·ªß th·ªùi gian qu√©t",
                    f"C·∫ßn qu√©t t·ªëi thi·ªÉu {SCAN_MIN_DURATION} gi√¢y.\n"
                    f"B·∫°n c·∫ßn qu√©t th√™m {remaining} gi√¢y n·ªØa.\n\n"
                    f"N·∫øu d·ª´ng b√¢y gi·ªù s·∫Ω KH√îNG xu·∫•t b√°o c√°o.\n"
                    f"B·∫°n c√≥ ch·∫Øc ch·∫Øn mu·ªën d·ª´ng kh√¥ng?"
                )

                if answer:
                    force_exit_no_report = True
                    break
                else:
                    continue
            else:
                break


        elif key == ord('v') and not ROI_ACTIVE:
            ROI_DRAWING = not ROI_DRAWING

            # CH·ªà xo√° khung khi T·∫ÆT v·∫Ω
            if not ROI_DRAWING:
                roi_start = None
                roi_end = None
                ROI_BOX = None

        elif key == ord('s') and ROI_BOX and not ROI_ACTIVE:
            global ROI_IMAGE_BUFFER
            if not ZONE_ID:
                ok = ask_student_id(root)

                # √âP c·ª≠a s·ªï OpenCV hi·ªán l·∫°i sau khi ƒë√≥ng Tkinter
                cv2.namedWindow(window_title)
                hwnd = win32gui.FindWindow(None, window_title)
                if hwnd:
                    win32gui.SetWindowPos(
                        hwnd,
                        win32con.HWND_TOPMOST,
                        0, 0, 0, 0,
                        win32con.SWP_NOMOVE | win32con.SWP_NOSIZE
                    )

                if not ok:
                    show_warning("Vui l√≤ng nh·∫≠p Student ID tr∆∞·ªõc khi qu√©t ROI.")
                    continue

            ROI_ACTIVE = True
            ROI_LOGS.clear()
            roi_scan_start_time = time.time()

            x1, y1, x2, y2 = ROI_BOX
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)

            roi_crop = frame[y1:y2, x1:x2]

            if roi_crop.size > 0:
                _, buffer = cv2.imencode(".png", roi_crop)
                ROI_IMAGE_BUFFER = buffer.tobytes()

            show_warning("B·∫ÆT ƒê·∫¶U QU√âT ROI (ƒê√É CH·ª§P ·∫¢NH ROI)")


        elif key == ord('e') and ROI_ACTIVE:
            # G·ª¨I TR·∫†NG TH√ÅI CU·ªêI C√ôNG N·∫æU ƒê·ª¶ ƒêI·ªÄU KI·ªÜN
            if ROI_STATE_TRACKER["state"] and ROI_STATE_TRACKER["start_time"]:
                now = time.time()
                duration = now - ROI_STATE_TRACKER["start_time"]

                if duration >= ABNORMAL_THRESHOLD:
                    duration = int(time.time() - ROI_STATE_TRACKER["start_time"])

                    now_dt = datetime.datetime.now()
                    start_time_str = (now_dt - datetime.timedelta(seconds=duration)).strftime('%H:%M:%S')

                    send_incident(
                        ROI_STATE_TRACKER["state"],
                        start_time_str,
                        duration
                    )

            ROI_ACTIVE = False
            ROI_STATE_TRACKER["state"] = None
            ROI_STATE_TRACKER["start_time"] = None

            root.after(0, export_roi_to_word)
            ROI_IMAGE_PATH = None
            ZONE_ID = None

        if ROI_DRAWING:
            roi_status_text = "V·∫Ω khung ROI: B·∫¨T"
            roi_status_color = (0, 255, 0)
        else:
            roi_status_text = "V·∫Ω khung ROI: T·∫ÆT"
            roi_status_color = (255, 0, 0)
        frame_stream = cv2.resize(frame.copy(), (int(WIDTH_SCR*0.4), int(HEIGHT_SCR*0.4)))
        with frame_lock:
            latest_frame = frame_stream.copy()
            
        new_w, new_h = int(WIDTH_SCR * scale_factor), int(HEIGHT_SCR * scale_factor)

        # ===== V·∫º ROI REALTIME (FULLSCREEN) =====

        # ƒêang k√©o chu·ªôt ‚Üí khung XANH DA TR·ªúI
        if ROI_DRAWING and not ROI_ACTIVE and roi_start and roi_end:
            cv2.rectangle(frame, roi_start, roi_end, (255,255,0), 2) # Xanh da tr·ªùi

        # ƒê√£ th·∫£ chu·ªôt ‚Üí khung V√ÄNG + ch·ªØ
        if ROI_BOX and ROI_DRAWING:
            x1, y1, x2, y2 = ROI_BOX
            cv2.rectangle(
                frame,
                (x1, y1),
                (x2, y2),
                (0, 255, 255) if not ROI_ACTIVE else (0, 255, 0),  # V√†ng
                2
            )

            cv2.putText(
                frame,
                f"ROI ACTIVE | HS-{ZONE_ID}" if ROI_ACTIVE else "ROI DRAWN",
                (x1, y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (0, 255, 255) if not ROI_ACTIVE else (0, 255, 0),
                2
            )

        cv2.imshow(window_title, cv2.resize(frame, (new_w, new_h)))
        
        fullscreen_icon = os.path.join(
            BASE_DIR,
            "Emotion + Posture Detector v3.0 Fullscreen Capture.ico"
        )

        if first_show:
            bring_window_to_front(window_title)
            set_opencv_window_icon(window_title, fullscreen_icon)
            first_show = False

        cv2.namedWindow(window_title)
        cv2.setMouseCallback(window_title, mouse_draw_roi_fullscreen)

        if first_show:
            bring_window_to_front(window_title)
            first_show = False
        else:
            # V·∫´n gi·ªØ c·ª≠a s·ªï CV2 lu√¥n tr√™n c√πng (n·∫øu c√≥)
            hwnd = win32gui.FindWindow(None, window_title)
            if hwnd: win32gui.SetWindowPos(hwnd, win32con.HWND_TOPMOST, 0, 0, 0, 0, win32con.SWP_NOMOVE | win32con.SWP_NOSIZE)
        
    # KHI V√íNG L·∫∂P K·∫æT TH√öC: D·ªçn d·∫πp
    cv2.destroyAllWindows()
    with thread_lock:
        is_running = False
        detection_thread = None # ƒê·∫∑t l·∫°i lu·ªìng ƒë·ªÉ c√≥ th·ªÉ ch·∫°y l·∫°i

    if len(DATA_LOGS) > 1 and not force_exit_no_report:
        root.after(100, analyze_and_export_csv) # Ch·∫°y h√†m xu·∫•t CSV tr√™n lu·ªìng ch√≠nh Tkinter


# --- GIAO DI·ªÜN CH√çNH (GUI CH·ªåN L·ª∞A) ---

def select_camera_and_run(cam_index):
    global root, current_mode, detection_thread
    if cam_index == -1:
        messagebox.showwarning("Ch∆∞a ch·ªçn", "Vui l√≤ng ch·ªçn m·ªôt camera.")
        return
    
    with thread_lock:
        if is_running:
            messagebox.showwarning("ƒêang ch·∫°y", "M·ªôt ch·∫ø ƒë·ªô qu√©t ƒëang ch·∫°y. Vui l√≤ng t·∫Øt c·ª≠a s·ªï qu√©t (ho·∫∑c nh·∫•n 'Q') tr∆∞·ªõc.")
            return

    current_mode = 'camera'
    # KH√îNG D√ôNG root.withdraw() n·ªØa

    # Hi·ªÉn th·ªã loading tr∆∞·ªõc khi ch·∫°y lu·ªìng detection
    show_loading_window("ƒêang kh·ªüi ƒë·ªông Camera...")
    
    # Ch·∫°y detection trong lu·ªìng ri√™ng
    detection_thread = Thread(target=run_detection_camera, args=(cam_index,), daemon=True)
    detection_thread.start()

def open_camera_selection_dialog():
    """Hi·ªÉn th·ªã c·ª≠a s·ªï ch·ªçn camera v√† l·ªõp tr∆∞·ªõc khi ch·∫°y ch·∫ø ƒë·ªô Camera."""
    global cam_window, camera_combo, class_combo, class_name, root

    # Ki·ªÉm tra xem c√≥ ƒëang ch·∫°y ch·∫ø ƒë·ªô n√†o kh√¥ng
    with thread_lock:
        if is_running:
            messagebox.showwarning("ƒêang ch·∫°y", 
                                   "M·ªôt ch·∫ø ƒë·ªô qu√©t ƒëang ch·∫°y. Vui l√≤ng t·∫Øt c·ª≠a s·ªï qu√©t (ho·∫∑c nh·∫•n 'Q') tr∆∞·ªõc.")
            return 
    
    cam_window = tk.Toplevel(root)
    cam_window.title("Ch·ªçn Camera v√† L·ªõp")
    # cam_window.attributes('-topmost', True)
    
    cameras = ["Ch·ªçn Camera..."] + list_cameras()
    tk.Label(cam_window, text="Vui l√≤ng ch·ªçn camera ƒë·ªÉ qu√©t:", font=("Arial", 10)).pack(pady=5)

    camera_combo = ttk.Combobox(cam_window, values=cameras, state="readonly", width=30)
    camera_combo.current(0)
    camera_combo.pack(pady=5)

    # --- Th√™m ComboBox ch·ªçn l·ªõp ---
    tk.Label(cam_window, text="Ch·ªçn l·ªõp h·ªçc:", font=("Arial", 10)).pack(pady=5)
    classes = ["L·ªõp 12A1", "L·ªõp 12A2", "L·ªõp 12A3", "L·ªõp 12A4", "L·ªõp 12A5"]
    class_combo = ttk.Combobox(cam_window, values=classes, state="readonly", width=30)
    class_combo.current(0)  # M·∫∑c ƒë·ªãnh ch·ªçn l·ªõp ƒë·∫ßu ti√™n
    class_combo.pack(pady=5)

    tk.Label(cam_window, text="H∆∞·ªõng d·∫´n v·∫Ω khung:", font=("Arial", 8)).pack(pady=1)
    tk.Label(cam_window, text="1. B·∫•m ph√≠m V ƒë·ªÉ b·∫≠t/t·∫Øt v·∫Ω khung", font=("Arial", 8)).pack(pady=1)
    tk.Label(cam_window, text="2. B·∫•m ph√≠m S ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√©t", font=("Arial", 8)).pack(pady=1)
    tk.Label(cam_window, text="3. B·∫•m ph√≠m E ƒë·ªÉ d·ª´ng qu√©t v√† xu·∫•t file", font=("Arial", 8)).pack(pady=1)

    def on_run():
        selected_index = camera_combo.current() - 1
        if selected_index < 0:
            messagebox.showwarning("Ch∆∞a ch·ªçn", "Vui l√≤ng ch·ªçn m·ªôt camera trong danh s√°ch.")
            return
        
        # L·∫•y l·ªõp h·ªçc ƒë∆∞·ª£c ch·ªçn
        global class_name
        class_name = class_combo.get()
        if not class_name:
            messagebox.showwarning("Ch∆∞a ch·ªçn", "Vui l√≤ng ch·ªçn m·ªôt l·ªõp h·ªçc.")
            return

        select_camera_and_run(selected_index)

    tk.Button(cam_window, text="M·ªü camera", command=on_run,
              bg="#007ACC", fg="white", font=("Arial", 10, "bold")).pack(pady=10)

    # CƒÉn gi·ªØa c·ª≠a s·ªï
    root.update_idletasks()
    cam_window.update_idletasks()
    x = root.winfo_x() + (root.winfo_width() - cam_window.winfo_reqwidth()) // 2
    y = root.winfo_y() + (root.winfo_height() - cam_window.winfo_reqheight()) // 2
    cam_window.geometry(f"+{x}+{y}")


def open_class_selection_dialog_for_fullscreen(on_confirm):
    """
    Hi·ªÉn th·ªã h·ªôp tho·∫°i ch·ªçn l·ªõp tr∆∞·ªõc khi ch·∫°y Fullscreen/Camera.
    on_confirm: callback ƒë∆∞·ª£c g·ªçi sau khi ch·ªçn l·ªõp th√†nh c√¥ng
    """
    global root, class_name

    class_window = tk.Toplevel(root)
    class_window.title("Ch·ªçn l·ªõp")
    class_window.resizable(False, False)
    class_window.grab_set()  # Kh√≥a c√°c c·ª≠a s·ªï kh√°c

    tk.Label(
        class_window,
        text="Vui l√≤ng ch·ªçn l·ªõp:",
        font=("Arial", 10, "bold")
    ).pack(pady=(12, 6))

    classes = [
        "Ch·ªçn l·ªõp...",
        "L·ªõp 12A1",
        "L·ªõp 12A2",
        "L·ªõp 12A3",
        "L·ªõp 12A4",
        "L·ªõp 12A5"
    ]

    class_combo = ttk.Combobox(
        class_window,
        values=classes,
        state="readonly",
        width=30
    )
    class_combo.current(0)
    class_combo.pack(pady=5)

    def confirm():
        global class_name
        selected = class_combo.get()

        if selected == "Ch·ªçn l·ªõp...":
            messagebox.showwarning(
                "Ch∆∞a ch·ªçn",
                "Vui l√≤ng ch·ªçn m·ªôt l·ªõp tr∆∞·ªõc khi ti·∫øp t·ª•c."
            )
            return

        class_name = selected  # G√ÅN BI·∫æN TO√ÄN C·ª§C
        class_window.destroy()

        if callable(on_confirm):
            on_confirm()

    tk.Button(
        class_window,
        text="X√°c nh·∫≠n",
        command=confirm,
        bg="#007ACC",
        fg="white",
        font=("Arial", 10, "bold"),
        width=18
    ).pack(pady=12)

    # ===== CƒÉn gi·ªØa c·ª≠a s·ªï =====
    root.update_idletasks()
    class_window.update_idletasks()

    x = root.winfo_x() + (root.winfo_width() - class_window.winfo_reqwidth()) // 2
    y = root.winfo_y() + (root.winfo_height() - class_window.winfo_reqheight()) // 2

    class_window.geometry(f"+{x}+{y}")

def start_fullscreen_capture():
    global root, current_mode, detection_thread
    
    with thread_lock:
        if is_running:
             messagebox.showwarning("ƒêang ch·∫°y", "M·ªôt ch·∫ø ƒë·ªô qu√©t ƒëang ch·∫°y. Vui l√≤ng t·∫Øt c·ª≠a s·ªï qu√©t (ho·∫∑c nh·∫•n 'Q') tr∆∞·ªõc.")
             return 
    
    def after_class_selected():
        global current_mode, detection_thread

        current_mode = 'screen'

        show_loading_window("ƒêang kh·ªüi ƒë·ªông Fullscreen Capture...")

        # Ch·∫°y detection trong lu·ªìng ri√™ng
        detection_thread = Thread(
            target=run_detection_fullscreen,
            daemon=True
        )
        detection_thread.start()

    open_class_selection_dialog_for_fullscreen(after_class_selected)



def stop_detection():
    # Ch·ªâ gi·ªØ l·∫°i c√°c bi·∫øn global c·∫ßn thi·∫øt cho vi·ªác d·ª´ng lu·ªìng v√† reset
    global is_running, detection_thread, broadcast_thread, root

    # 1. D·ª´ng lu·ªìng qu√©t
    with thread_lock:
        is_running = False
        
    # 2. G·ªåI analyze_and_export_csv tr√™n lu·ªìng ch√≠nh Tkinter
    # D√π stop_detection ƒë∆∞·ª£c g·ªçi t·ª´ ƒë√¢u, ta d√πng root.after ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n cho Tkinter calls
    if root:
        root.after(100, analyze_and_export_csv) 
        
    # 3. D·ªçn d·∫πp lu·ªìng 
    detection_thread = None
    broadcast_thread = None

    print("Qu√° tr√¨nh d·ª´ng ƒë√£ ho√†n t·∫•t.")

# S·ª≠a lu√¥n h√†m on_closing() ƒë·ªÉ n√≥ g·ªçi stop_detection()
def on_closing():
    global is_running, root
    with thread_lock:
        is_running_local = is_running

    if is_running_local:
        if messagebox.askyesno("Tho√°t", "Ch∆∞∆°ng tr√¨nh qu√©t ƒëang ch·∫°y. B·∫°n c√≥ mu·ªën d·ª´ng v√† tho√°t kh√¥ng?"):
            stop_detection() # D·ª´ng lu·ªìng qu√©t (s·∫Ω t·ª± ƒë·ªông g·ªçi analyze_and_export_csv)
            root.quit() # Tho√°t kh·ªèi Tkinter
    else:
        root.quit()

root = tk.Tk()
root.title("L·ª±a Ch·ªçn Ch·∫ø ƒê·ªô")
if os.path.exists(icon_path):
    root.iconbitmap(icon_path)
    
# G√°n h√†m x·ª≠ l√Ω s·ª± ki·ªán ƒë√≥ng c·ª≠a s·ªï (S·ª≠a l·ªói ·ªü ƒë√¢y)
root.protocol("WM_DELETE_WINDOW", on_closing) 

window_width = 550
# Chi·ªÅu cao ƒë√£ ƒë∆∞·ª£c tƒÉng l√™n ƒë·ªÉ ch·ª©a 4 n√∫t
window_height = 300
screen_width = root.winfo_screenwidth()
screen_height = root.winfo_screenheight()
x = (screen_width // 2) - (window_width // 2)
y = (screen_height // 2) - (window_height // 2)
root.geometry(f'{window_width}x{window_height}+{x}+{y}')
root.resizable(False, False)


tk.Label(root, text="CH·ªåN CH·∫æ ƒê·ªò QU√âT BI·ªÇU C·∫¢M KHU√îN M·∫∂T/T∆Ø TH·∫æ:", 
         font=("Arial", 12, "bold")).pack(pady=10)

tk.Button(root, text="üì∑ QU√âT B·∫∞NG CAMERA", 
          command=open_camera_selection_dialog,
          width=30, height=2, bg="#007ACC", fg="white", font=("Arial", 10, "bold")).pack(pady=5)

tk.Button(root, text="üñ•Ô∏è QU√âT B·∫∞NG M√ÄN H√åNH",
          command=start_fullscreen_capture,
          width=30, height=2, bg="#FF9800", fg="white", font=("Arial", 10, "bold")).pack(pady=5)

tk.Button(root, text="AI Smart Monitor",
          command=open_aismartmonitor,
          width=30, height=2, bg="#216C71", fg="white", font=("Arial", 10, "bold")).pack(pady=5)

# N√∫t t√πy ch·ªçn th∆∞ m·ª•c log
tk.Button(root, text="üìÅ Xu·∫•t file log t·∫°i...",
          command=set_log_directory,
          width=30, height=2, bg="#38505D", fg="white", font=("Arial", 10, "bold")).pack(pady=5)

root.mainloop()
